Representation Learning for Clustering: A Statistical Framework

INTRODUCTION

Clustering can be thought as the task of automatically dividing a set of objects into "coherent" subsets. This definition is not concrete, but its vagueness allows it to serve as an umbrella term for a wide diversity of algorithmic paradigms. Clustering algorithms are being routinely applied in a huge variety of fields.

Given a dataset that needs to be clustered for some application, one can choose among a variety of different clustering algorithms, along with different pre-processing techniques, that are likely to result in dramatically different answers. It is therefore critical to incorporate prior knowledge about the data and the intended semantics of the clustering into the process of picking a clustering algorithm (or, clustering model selection). Regretfully, there seem to be no systematic tool for incorporation of domain expertise for clustering model selection, and such decisions are usually being made in embarrassingly ad hoc ways. This paper aims to address that critical deficiency in a formal statistical framework.

We approach the challenge by considering a scenario in which the domain expert (i.e., the intended user of the clustering) conveys her domain knowledge by providing a clustering of a small random subset of her data set. For example, consider a big customer service center that wishes to cluster incoming requests into groups to streamline their handling. Since the data base of requests is too large to be organized manually, the service center wishes to employ a clustering program. As the clustering designer, we would then ask the service center to pick a random sample of requests, manually cluster them, and show us the resulting grouping of that sample. The clustering tool then uses that sample clustering to pick a clustering method that, when applied to the full data set, will result in a clustering that follows the patterns demonstrated by that sample clustering. We address this paradigm from a statistical machine learning perspective. Aiming to achieve generalization guaranties for such an approach, it is essential to introduce some inductive bias. We do that by restricting the clustering algorithm to a predetermined hypothesis class (or a set of concrete clustering algorithms).

In a recent Dagstuhl workshop, [\cite=blum2014Approximation] proposed to do that by fixing a clustering algorithm, say k-means, and searching for a metric over the data under which k-means optimization yields a clustering that agrees with the training sample clustering. One should note that, given any domain set X, for any k-partitioning P of X, there exists some distance function dP over X such that P is the optimal k-means clustering solution to the input (X,dP). Consequently, to protect against potential overfitting, the class of potential distance functions should be constrained. In this paper, we provide (apparently the first) concrete formal framework for such a paradigm, as well as a generalization analysis of this approach.

In this work we focus on center based clustering - an important class of clustering algorithms. In these algorithms, the goal is to find a set of "centers" (or prototypes), and the clusters are the Voronoi cells induced by this set of centers. The objective of such a clustering is to minimize the expected value of some monotonically increasing function of the distances of points to their cluster centers. The k-means clustering objective is arguably the most popular clustering paradigm in this class. Currently, center-based clustering tools lack a vehicle for incorporating domain expertise. Domain knowledge is usually taken into account only through an ad hoc choice of input data representation. Regretfully, it might not be realistic to require the domain expert to translate sufficiently elaborate task-relevant knowledge into hand-crafted features.

As a model for learning representations, we assume that the user-desirable clustering can be approximated by first mapping the sample to some Euclidean (or Hilbert) space and then performing k-means clustering in the mapped space (or equivalently, replacing the input data metric by some kernel and performing center-based clustering with respect to that kernel). However, the clustering algorithm is supposed to learn a suitable mapping based on the given sample clustering.

The main question addressed in this work is that of the sample complexity: what is the size of a sample, to be clustered by the domain expert, that suffices for finding a close-to-optimal mapping (i.e., a mapping that generalizes well on the test data)? Intuitively, this sample complexity depends on the richness of the class of potential mappings that the algorithm is choosing from. In standard supervised learning, there are well established notions of capacity of hypothesis classes (e.g., VC-dimension) that characterize the sample complexity of learning. This paper aims to provide such relevant notions of capacity for clustering.

Previous Work

In practice, there are methods that use some forms of supervision for clustering. These methods are sometimes called "semi-supervised clustering" ([\cite=basu2002semi] [\cite=basu2004probabilistic] [\cite=kulis2009semi]). The most common method to convey such supervision is through a set of pairwise must/cannot-link constraints on the instances ([\cite=wagstaff2001constrained]). A common way of using such information is by changing the objective of clustering so that violations of these constraints are penalized ([\cite=demiriz1999semi] [\cite=law2005model] [\cite=basu2008constrained]). Another approach, which is closer to ours, keeps the clustering optimization objective fixed, and instead, searches for a metric that best fits given constraints. The metric is learned based on some objective function over metrics ([\citep=xing2002distance] [\citep=alipanahi2008distance]), so that pairs of instances marked must-link will be close in the new metric space (and cannot-link pairs be considered as far apart). The two above approaches can also be integrated ([\cite=bilenko2004integrating]). However, these objective functions are usually rather ad hoc. In particular, it is not clear in what sense they are compatible with the adopted clustering algorithm (such as k-means clustering).

A different approach to the problem of communicating user expertise for the purpose of choosing a clustering tool is discussed in [\cite=ackerman2010towards]. They considered a set of properties, or requirements, for clustering algorithms, and investigated which of those properties hold for various algorithms. The user can then pick the right algorithm based on the requirements that she wants the algorithm to meet. However, to turn such an approach into a practically useful tool, one will need to come up with properties that are relevant to the end user of clustering -a goal that is still far from being reached.

Statistical convergence rates of sample clustering to the optimal clustering, with respect to some data generating probability distribution, play a central role in our analysis. From that perspective, most relevant to our paper are results that provide generalization bounds for k-means clustering. [\cite=ben2007framework] proposed the first dimension-independent generalization bound for k-means clustering based on compression techniques. [\cite=biau2008performance] tightened this result by an analysis of Rademacher complexity. [\cite=maurer2010dimensional] investigated a more general framework, in which generalization bounds for k-means as well as other algorithms can be obtained. It should be noted that these results are about the standard clustering setup (without any supervised feedback), where the data representation is fixed and known to the clustering algorithm.

Contributions

Our first contribution is to provide a statistical framework to analyze the problem of learning representation for clustering. We assume that the expert has some implicit target clustering of the dataset in his mind. The learner however, is unaware of it, and instead has to select a mapping among a set of potential mappings, under which the result of k-means clustering will be similar to the target partition. An appropriate notion of loss function is introduced to quantify the success of the learner. Then, we define the analogous notion of PAC-learnability for the problem of learning representation for clustering.

The second contribution of the paper is the introduction of a combinatorial parameter, a specific notion of the capacity of the class of mappings, that determines the sample complexity of the clustering learning tasks. This combinatorial notion is a multivariate version of pseudo-dimension of a class of real-valued mappings. We show that there is uniform convergence of empirical losses to the true loss, over any class of embeddings, F, at a rate that is determined by the proposed dimension of that F. This implies that any empirical risk minimization algorithm (ERM) will successfully learn such a class from sample sizes upper bounded by those rates. Finally, we analyze a particular natural class -the class of linear mappings from [formula] to [formula]- and show that a roughly speaking, sample size of [formula] is sufficient to guarantee an ε-optimal representation.

The rest of this paper is organized as follows: Section [\ref=sec:setting] defines the problem setting. Then in Section  [\ref=sec:analysis], we investigate ERM-type algorithms and show that, "uniform convergence" is sufficient for them to work. Furthermore, this section presents the uniform convergence results and the proof of an upper bound for the sample complexity. Finally, we conclude in section [\ref=sec:conc] and provide some directions for future work.

PROBLEM SETTING

Preliminaries

Let X be a finite domain set. A k-clustering of X is a partition of X into k subsets. If C is a k-clustering, we denote the subsets of the partition by C1,...,Ck, therefore we have C = {C1,..,Ck}. Let πk denote the set of all permutations over

[formula]

denotes {1,2,...,k}. The clustering difference between two clusterings, C1 and C2, with respect to X is defined by

[formula]

where |.| and Δ denote the cardinality and the symmetric difference of sets respectively. For a sample S  ⊂  X, and C1 (a partition of X), we define [formula] to be a partition of S induced by C1, namely [formula]. Accordingly, the sample-based difference between two partitions is defined by

[formula]

Let f be a mapping from X to [formula], and [formula] be a vector of k centers in [formula]. The clustering defined by (f,μ) is the partition over X induced by the μ-Voronoi partition in [formula]. Namely,

[formula]

[formula]

The k-means cost of clustering X with a set of centers [formula] and with respect to a mapping f is defined by

[formula]

The k-means clustering algorithm finds the set of centers μfX that minimize this cost. In other words,

[formula]

Also, for a partition C and mapping f, we can define the cost of clustering as follows.

[formula]

For a mapping f as above, let CfX denote the k-means clustering of X induced by f, namely

[formula]

The difference between two mappings f1 and f2 with respect to X is defined by the difference between the result of k-means clustering using these mappings. Formally,

[formula]

The following proposition shows the "k-richness" property of k-means objective.

Let X be a domain set. For every k-clustering of X, C, and every [formula], there exist a mapping [formula] such that CgX  =  C.

The mapping g can be picked such that it collapses each cluster Ci into a single point in [formula] (and so the image of X under mapping g will be just k single points in [formula]). The result of k-means clustering under such mapping will be C.

In this paper, we investigate the transductive setup, where there is a given data set, known to the learner, that needs to be clustered. Clustering often occurs as a task over some data generating distribution (e.g., [\cite=von2005towards]). The current work can be readily extended to that setting. However, in that case, we assume that the clustering algorithm gets, on top of the clustered sample, a large unclustered sample drawn form that data generating distribution.

Formal Problem Statement

Let C* be the target k-clustering of X. A (supervised) representation learner for clustering, takes as input a sample S  ⊂  X and its clustering, [formula], and outputs a mapping f from a set of potential mappings F. In the following, PAC stands for the notion of "probably approximately correct".

Let F be a set of mappings from X to [formula]. A representation learning algorithm A is a PAC-SRLK with sample complexity [formula] with respect to F, if for every (ε,δ)∈(0,1)2, every domain set X and every clustering of X, C*, the following holds:

if S is a randomly (uniformly) selected subset of X of size at least mF(ε,δ), then with probability at least 1 - δ

[formula]

where [formula], is the output of the algorithm.

This can be regarded as a formal PAC framework to analyze the problem of learning representation for k-means clustering. The learner is compared to the best mapping in the class F.

A natural question is providing bounds on the sample complexity of PAC-SRLK with respect to F. Intuitively, for richer classes of mappings, we need larger clustered samples. Therefore, we need to introduce an appropriate notion of "capacity" for F and bound the sample complexity based on it. This is addressed in the next sections.

ANALYSIS AND RESULTS

Empirical Risk Minimization

In order to prove an upper bound for the sample complexity of representation learning for clustering, we need to consider an algorithm, and prove a sample complexity bound for it. Here, we show that any ERM-type algorithm can be used for this purpose. Therefore, we will be able to prove an upper bound for the sample complexity of PAC-SRLK.

Let F be a class of mappings and X be the domain set. A TERM learner for F takes as input a sample S  ⊂  X and its clustering Y and outputs:

[formula]

Note that we call it transductive, because it is implicitly assumed that it has access to unlabeled dataset (i.e., X). A TERM algorithm goes over all mappings in F and selects the mapping which is the most consistent mapping with the given clustering: the mapping under which if we perform k-means clustering of X, the sample-based Δ-difference between the result and Y is minimized.

Note that we are not studying this algorithm as a computational tool; we only use it to show an upper bound for the sample complexity.

Intuitively, this algorithm will work well when the empirical Δ-difference and the true Δ-difference of the mappings in the class are close to each other. In this case, by minimizing the empirical difference, the algorithm will automatically minimize the true difference as well. In order to formalize this idea, we define the notion of "representativeness" of a sample.

Let F be a class of mappings from X to [formula]. A sample S is ε-representative with respect to F, X and the clustering C*, if for every f∈F the following holds

[formula]

The following theorem shows that for the TERM algorithm to work, it is sufficient to supply it with a representative sample.

Let F be a set of mappings from X to [formula]. If S is an [formula]-representative sample with respect to X, F and C* then

[formula]

where [formula] and [formula].

Using [formula]-representativeness of S and the fact that [formula] is the empirical minimizer of the loss function, we have

[formula]

[formula]

[formula]

[formula]

Therefore, we just need to provide an upper bound for the sample complexity of uniform convergence: "how many instances do we need to make sure that with high probability our sample is ε-representative?"

Classes of Mappings with a Uniqueness Property

In general, the solution to k-means clustering may not be unique. Therefore, the learner may end up with finding a mapping that corresponds to multiple different clusterings. This is not desirable, because in this case, the output of the learner will not be interpretable. Therefore, it is reasonable to choose the class of potential mappings in a way that it includes only the mappings under which the solution is unique.

In order to make this idea concrete, we need to define an appropriate notion of uniqueness. We use a notion similar to the one introduced by [\cite=balcan2009approximate] with a slight modification.

We say that k-means clustering for domain X under mapping [formula] has a (η,ε)-unique solution, if every η-optimal solution of the k-means cost is ε-close to the optimal solution. Formally, the solution is (η,ε)-unique if for every partition P that satisfies

[formula]

would also satisfy

[formula]

In the degenerate case where the optimal solution to k-means is not unique itself (and so CfX is not well-defined), we say that the solution is not (η,ε)-unique.

It can be noted that the definition of (η,ε)-uniqueness not only requires the optimal solution to k-means clustering to be unique, but also all the "near-optimal" minimizers of the k-means clustering cost should be "similar". This is a natural strengthening of the uniqueness condition, to guard against cases where there are η0-optimizers of the cost function (for arbitrarily small η0) with totally different solutions.

Now that we have a definition for uniqueness, we can define the set of mappings for X under which the solution is unique. We say that a class of mappings F has (η,ε)-uniqueness property with respect to X, if every mapping in F has (η,ε)-uniqueness property over X.

Note that given an arbitrary class of mappings F, we can find a subset of it that satisfies (η,ε)-uniqueness property over X. Also, as argued above, this subset is the useful subset to work with. Therefore, in the rest of the paper, we investigate learning for classes with (η,ε)-uniqueness property. In the next section, we prove uniform convergence results for such classes.

Uniform Convergence Results

In Section 3.1, we defined the notion of ε-representative samples. Also, we proved that if a TERM algorithm is fed with such a representative sample, it will work satisfactorily. The most technical part of the proof is then about the question "how large should be the sample in order to make sure that with high probability it is actually a representative sample?"

In order to formalize this notion, let F be a set of mappings from a domain X to (0,1)n. Define the sample complexity of uniform convergence, mUCF(ε,δ), as the minimum number m such that for every fixed partition C*, if S is a randomly (uniformly) selected subset of X with size m, then with probability at least 1 - δ, for all f∈F we have

[formula]

The technical part of this paper is devoted to provide an upper bound for this sample complexity.

Preliminaries

Let F be a set of mappings from X to (0,1)n. A subset   ⊂  F is called an ε-cover for F with respect to the metric d(.,.) if for every f∈F there exists ∈ such that d(f,)  ≤  ε. The covering number, N(F,d,ε) is the size of the smallest ε-cover of F with respect to d.

In the above definition, we did not specify the metric d. In our analysis, we are interested in the L1 distance with respect to X, namely:

[formula]

Note that the mappings we consider are not real-valued functions, but their output is an n-dimensional vector. This is in contrast to the usual analysis used for learning real-valued functions. If f1 and f2 are real-valued, then L1 distance is defined by

[formula]

We will prove sample complexity bounds for our problem based on the L1-covering number of the set of mappings. However, it will be beneficial to have a bound based on some notion of capacity, similar to VC-dimension, as well. This will help in better understanding and easier analysis of sample complexity of different classes. While VC-dimension is defined for binary valued functions, we need a similar notion for functions with outputs in [formula]. For real-valued functions, we have such notion, called pseudo-dimension ([\cite=pollard1984convergence]).

(Pseudo-Dimension) Let F be a set of functions from X to [formula]. Let [formula] be a subset of X. Then S is pseudo-shattered by F if there are real numbers [formula] such that for every b∈{0,1}m, there is a function fb∈F with sgn(fb(xi) - ri) = bi for i∈[m]. Pseudo dimension of F, called Pdim(F), is the size of the largest shattered set.

It can be shown (e.g., Theorem 18.4. in [\cite=anthony2009neural]) that for a real-valued class F, if Pdim(F)  ≤  q then log N(F,dXL1,ε)  =  O(q) where O() hides logarithmic factors of [formula]. In the next sections, we will generalize this notion to [formula]-valued functions.

Reduction to Binary Hypothesis Classes

Let f1,f2∈F be two mappings and σ be a permutation over

[formula]

, then

[formula]

For a set S, and a binary function h(.), let [formula]. We now show that a uniform convergence result with respect to HF is sufficient to have uniform convergence for the Δ-difference function. Therefore, we will be able to investigate conditions for uniform convergence of HF rather than the Δ-difference function.

Let X be a domain set, F be a set of mappings, and HF be defined as above. If S  ⊂  X is such that

[formula]

then S will be ε-representative with respect to F, i.e., for all f1,f2∈F we will have

[formula]

[formula]

[formula]

[formula]

[formula]

The fact that HF is a class of binary-valued functions enables us to provide sample complexity bounds based on VC-dimension of this class. However, providing bounds based on VC-Dim(HF) is not sufficient, in the sense that it is not convenient to work with the class HF. Instead, it will be nice if we can prove bounds directly based on the capacity of the class of mappings, F. In the next section, we address this issue.

L1-Covering Number and Uniform Convergence

The classes introduced in the previous section, HF and HFσ, are binary hypothesis classes. Also, we have shown that proving a uniform convergence result for HF is sufficient for our purpose. In this section, we show that a bound on the L1 covering number of F is sufficient to prove uniform convergence for HF.

In Section 3.2, we argued that we only care about the classes that have (η,ε)-uniqueness property. In the rest of this section, assume that F is a class of mappings from X to (0,1)n that satisfies (η,ε)-uniqueness property.

Let f1,f2∈F. If [formula] then ΔX(f1,f2)  <  2ε

We leave the proof of this lemma for the appendix, and present the next lemma.

Let HF be defined as in the previous section. Then,

[formula]

Let [formula] be the [formula]-cover corresponding to the covering number [formula]. Based on the previous lemma, [formula] is a 2ε-cover for HFσ. But we have only k! permutations of

[formula]

Basically, this means that if we have a small L1 covering number for the mappings, we will have the uniform convergence result we were looking for. The following theorem proves this result.

Let F be a set of mappings with (η,ε)-uniqueness property. Then there for some constant α we have

[formula]

Following the previous lemma, if we have a small L1-covering number for F, we will also have a small covering number for HF as well. But based on standard uniform convergence theory, if a hypothesis class has small covering number, then it has uniform convergence property. More precisely, (e.g., Theorem 17.1 in [\cite=anthony2009neural]) we have:

[formula]

Applying Lemma 2 to the above proves the result.

Bounding L1-Covering Number

In the previous section, we proved if the L1 covering number of the class of mappings is bounded, then we will have uniform convergence. However, it is desirable to have a bound with respect to a combinatorial dimension of the class (rather than the covering number). Therefore, we will generalize the notion of pseudo-dimension for the class of mappings that take value in [formula].

Let F be a set of mappings form X to [formula]. For every mapping f∈F, define real-valued functions [formula] such that [formula]. Now let Fi  =  {fi:f∈F}. This means that [formula] are classes of real-valued functions. Now we define pseudo-dimension of F as follow.

[formula]

Let F be a set of mappings form X to [formula]. If Pdim(F)  ≤  q then log N(F,dXL1,ε)  =  O(q) where O() hides logarithmic factors.

The result follows from the corresponding result for bounding covering number of real-valued functions based on pseudo-dimension mentioned in the preliminaries section. The reason is that we can create a cover by composition of the [formula]-covers of all Fi. However, this will at most introduce a factor of n in the logarithm of the covering number.

Therefore, we can rewrite the result of the previous section in terms of pseudo-dimension.

Let F be a class of mappings with (η,ε)-uniqueness property. Then

[formula]

where O() hides logarithmic factors of k and [formula].

Sample Complexity of PAC-SRLK

In Section 3.1, we showed that uniform convergence is sufficient for a TERM algorithm to work. Also, in the previous section, we proved a bound for the sample complexity of uniform convergence. The following theorem, which is the main technical result of this paper, combines these two and provides a sample complexity upper bound for PAC-SRLK framework.

Let F be a class of (η,ε)-unique mappings. Then the sample complexity of learning representation for k-means clustering with respect to F is upper bounded by

[formula]

where O hides logarithmic factors of k and [formula].

The proof is done by combining Theorems 1 and 4.

The following result shows an upper bound for the sample complexity of learning linear mappings (or equivalently, Mahalanobis metrics).

Let F be a set of (η,ε)-unique linear mappings from [formula] to [formula]. Then we have

[formula]

It is a standard result that the pseudo-dimension of a vector space of real-valued functions is just the dimensionality of the space (in our case d1) (e.g., Theorem 11.4 in [\cite=anthony2009neural]). Also, based on our definition of Pdim for [formula]-valued functions, it should scale by a factor of d2.

CONCLUSIONS AND OPEN PROBLEMS

In this paper we provided a formal statistical framework for learning the representation (i.e., a mapping) for k-means clustering based on supervised feedback. The learner, unaware of the target clustering of the domain, is given a clustering of a sample set. The learner's task is then finding a mapping function [formula] (among a class of mappings) under which the result of k-means clustering of the domain is as close as possible to the true clustering. This framework was called PAC-SRLK.

A notion of ε-representativeness was introduced, and it was proved that any ERM-type algorithm that has access to such a sample will work satisfactorily. Finally, a technical uniform convergence result was proved to make sure that a large enough sample is (with high probability) ε-representative. This was used to prove an upper bound for the sample complexity of PAC-SRLK based on covering numbers of the set of mappings. Furthermore, a notion of pseudo-dimension for the class of mappings was defined, and the sample complexity was upper bounded based on it.

Note that in the analysis, the notion of (η,ε)-uniqueness (similar to that of [\cite=balcan2009approximate]) was used and it was argued that it is reasonable to require the learner to output a mapping under which the solution is "unique" (because otherwise the output of k-means clustering would not be interpretable). Therefore, in the analysis, we assumed that the class of potential mappings has the (η,ε)-uniqueness property.

It can be noted that we did not analyze the computational complexity of algorithms for PAC-SRLK framework. We leave this analysis to the future work. We just note that a similar notion of uniqueness proposed by [\cite=balcan2009approximate] resulted in being able to efficiently solve the k-means clustering algorithm.

One other observation is that representation learning can be regarded as a special case of metric learning; because for every mapping, we can define a distance function that computes the distance in the mapped space. In this light, we can make the problem more general by making the learner find a distance function rather than a mapping. This is more challenging to analyze, because we do not even know a generalization bound for center-based clustering under general distance functions. An open question will be providing such general results.

APPENDIX

Proof of Lemma 1. Let F:X  ↦  (0,1)n be a set of mappings that have (η,ε)-uniqueness property. Let f1,f2∈F and [formula]. We need to prove that ΔX(f1,f2)  <  2ε. In order to prove this, note that due to triangular inequality, we have

[formula]

Therefore, it will be sufficient to show that each of the Δ-terms above is smaller than ε. We start by proving a useful lemma.

Let f1,f2∈F and [formula]. Let μ be an arbitrary set of k centers in (0,1)n. Then

[formula]

[formula]

[formula]

[formula]

[formula]

[formula]

Now we are ready to prove that the first Δ-term is smaller than ε, i.e., ΔX(Cf1(μf1),Cf1(μf2))  <  ε. But to do so, we only need to show that COSTX(f1,μf2)  -  COSTX(f1,μf1)  <  η; because in that case, due to (η,ε)-uniqueness property of f1, the result will follow. Now, using Lemma 3, we have

[formula]

[formula]

[formula]

[formula]

[formula]

where in the first and the last line we used Lemma 3.

Finally, we need to prove the second Δ-inequality, i.e., ΔX(Cf1(μf2),Cf2(μf2))  ≤  ε. Assume contrary. But based on (η,ε)-uniqueness property of f2, we conclude that COSTX(f2,Cf1(μf2))  -  COSTX(f2,Cf2(μf2))  ≥  η. In the following, we prove that this cannot be true, and hence a contradiction.

Let [formula]. Then, based on the boundedness of f1(x),f2(x) and we have:

[formula]

[formula]

[formula]

[formula]

[formula]

[formula]

[formula]