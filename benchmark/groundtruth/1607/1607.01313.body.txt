Scenario-based decision-making for power systems investment planning

Introduction: decision making in uncertain environments

Planning in power systems relies on many uncertainties. Some of them, originating in nature or in consumption, can be tackled through probabilities [\citep=RTEForecast] [\citep=pinson2013renewable] [\citep=siqueira2006stochastic] [\citep=Vassena2003]; others, such as technology evolution, geopolitics or CO2 penalization laws, are somewhere between stochastic and adversarial:

Climate: The United Nations Climate Change Conference, COP21, aims at achieving a new universal agreement on climate agreement, which is an issue of cooperation and competition.

Uranium supply: India has been using imported enriched uranium from Russia since 2001. In 2004, Russia deferred to the Nuclear Suppliers' Group and declined to supply further uranium for India's reactors. The uranium supply was not resumed until the end of 2008 (after the refurbishment was finished). Now, Russia is already supplying the India's first large nuclear power plant under a Russian-financed 3 billion contract; and in 2014, Russia agreed to help building 10 nuclear reactors in India.

Curtailment risk: Wind and solar curtailment may occur for several reasons including transmission congestion (or local network constraints), global oversupply and operational issues [\citep=lew2013wind]. Each type of curtailment occurs with different frequencies depending on the generation and electrical characteristics of the regional and local systems. Another example is the risk of terrorism in the congested traffic, which cannot be represented by any stochastic model.

Geopolitical implications: Affected by the dollar, geopolitical and other factors, at the beginning of 2008 the international crude oil prices rose sharply. Another example is the Ukraine Crisis, which made Europe consider seriously adjusting its energy policy to reduce its dependence on imported energy supply.

Handling such uncertainties is a challenge. For example, how should we modelize the risk of gas curtailment in Europe, and the evolution of oil prices ? We discuss existing methodologies in Section [\ref=decision:clas]. Section [\ref=compatools] compares them. Section [\ref=nashelec:spanas] describes our proposed approach. In particular, Section [\ref=nashelec:method] summarizes our method. Experiments are provided in Section [\ref=nashelec:real]. Section [\ref=nashelec:conc] concludes.

State of the art: decision with uncertainties

The notations are as follows: K is the number of possible policies. S is the number of possible scenarios. R is the matrix of rewards and the associated reward function (Rk,s = R(k,s)), i.e. R(k,s) is the reward when applying policy [formula] in case the outcome of uncertainties is [formula]. The reward function is also called a utility function or a payoff function. A strategy (a.k.a. policy) is a random variable k with values in K. A mixed strategy is a probability distribution of possible policies; this is the general case of a strategy. A pure strategy is a deterministic policy, i.e. it is a mixed strategy with probability 1 for one element, others having probability 0. The exploitability of a (deterministic or randomized) strategy k is

[formula]

We refer to the choice of s as Nature's choice. This does not mean that only natural effects are involved; geopolitics and technological uncertainties are included. k is chosen by us. In fact, natural phenomena can usually be modelized with probabilities, and are included through random perturbations - they are not the point in this work - contrarily to climate change uncertainties.

Scenario-based planning

Maybe the most usual solution consists in selecting a small set [formula] of possible s, assumed to be most realistic. Then, for each sj, an optimal ki is obtained. The human then checks the matrix of the R(ki,sj) for i and j in [formula]. Variants of this approach are studied in scenario planning [\citep=powerplanning1] [\citep=powerplanning2] [\citep=artuncertainties]. [\cite=plenty2] provides examples with more than 1000 scenarios. When optimizing the transmission network, we must take into account the future installation of power plants, for which there are many possible scenarios - in particular, the durations involved in power plant building are not necessarily larger than constants involved in big transmission lines. The scenarios involving large wind farms, or large nuclear power plants, lead to very specific constraints depending on their capacities and locations.

Wald criterion

The Wald criterion[\citep=wald] consists in optimizing in the worst case scenario. For a maximization problem, the Wald-value is

[formula]

and the recommended policy is k realizing the max . We choose a policy which provides the best solution (maximal reward) for the worst scenario. Wald's maximin model provides a reward which is guaranteed in all cases. Implicitly, it assumes that Nature will make its decision in order to bother us, and, in a more subtle manner, Nature will make its decision while knowing what we are going to decide. It is hard to believe, for example, that the ultimate technological limit of photovoltaic units will be worse if we decide to do massive investments in solar power. Therefore, Wald's criterion is too conservative in many cases; hence the design of the Savage criterion.

Savage criterion

The Savage-value[\citep=savage] is:

[formula]

where [formula]. The Savage criterion is an application of the Wald maximin model to the regret. Contrarily to Wald's criterion, it does not focus on the worst scenario. Its interpretation is that we optimize the guaranteed loss compared to an anticipative choice (anticipative in the sense: aware of all future outcomes) of decision. On the other hand, Nature still makes its decision after us, and has access to our decision before making its decision - Nature, in this model, can still decide to reduce the technological progress of wind turbines just because we have decided to do massive investments in wind power.

Nash equilibria

The principle of the Nash equilibrium is that contrarily to what is assumed in Wald's criterion (Eq. [\ref=eq:wald]), there is no reason for Nature (the opponent) to make a decision us, and to know what we have decided. The Nash-value v is As a mixed strategy is used, the fact that the maximum is written before the min does not change the result [\citep=neumann1928zur]; v is also equal to

[formula]

where r.v. stands for "random variable". The exploitability (Eq. [\ref=eq:expl]) of a (possibly mixed) strategy k is equivalent to A Nash strategy is a strategy with exploitability equal to 0. A Nash strategy always exists; it is not necessarily unique. A Nash equilibrium, for a finite-sum problem, is a pair of Nash strategies for us and for Nature respectively. In the general case, a Nash strategy is not pure. Criteria for Nash equilibria corresponds to Nature and us making decision privately, i.e. without knowing what each other will do. In this sense, it is more intuitive than other criteria.

Other decision tools

Other possible tools for partially adversarial decision making are multi-objective optimization (i.e. for each s, there is one objective function k  ↦  R(k,s)) and possibilistic reasoning [\citep=possibility]. These tools rely intensively on human experts, a priori (selection of scenarios) or a posteriori (selection in the Pareto set).

Comparison between various decision tools

Let us compare the various discussed policies, where K is the number of possible investment policies, S is the number of scenarios, K' is the number of displayed policies, S' is the number of displayed policies; we provide an overview in Table [\ref=decision:comp]. We see that the Nash approach (at least with the algorithms reaching the bound mentioned in the table) has a lower computational cost and some advantages in terms of modeling; Nature makes its decision privately (which means we do not know the uncertainties), but not with access to our decisions. On the other hand, its output is stochastic, which might be a drawback for users.

Our proposal: NashUncertaintyDecision

Our proposed tool is as follows: (i) We use Nash equilibria, for their principled nature and (as discussed later) low computational cost in large scale settings. (ii) We compute the equilibria thanks to adversarial bandit algorithms, as detailed in the next section. (iii) We use sparsity, for (i) improving the precision (ii) reducing the number of pure strategies in our recommendation. The resulting algorithm has the following advantage:

It is fast; this is not intuitive, but Nash equilibria, in spite of the complex theories behind this concept, can be approximated quickly, without computing the entire matrix of R(k,s). A pioneering work in this direction was [\cite=grigoriadis]; within logarithmic terms and dependency in the precision, the cost is roughly the square root of the size of the matrix.

It naturally provides a submatrix of R(k,s), for the best k and the most critical s.

We believe that such outcomes are natural tools for including in platforms for simulating large scale power systems involving huge uncertainties.

The algorithmic technology under the hood: computing Nash equilibria with adversarial bandit algorithms

For the computational cost issue for computing Nash equilibria, there exist algorithms reaching approximate solutions much faster than the exact linear programming approach [\citep=Stengel02computeequilibria]. Some of these fast algorithms are based on the bandit formalism. The Multi-Armed Bandit (MAB) problem [\citep=lairobbins] [\citep=katehakis1987multi] [\citep=auer95gambling] is a model of exploration/exploitation trade-offs, aimed at optimizing the expected payoff. Let us define an adversarial multi-armed bandit with [formula] (K > 1) arms and let K denote the set of arms. Let [formula] denote the set of time steps, with [formula] a finite time horizon. At each time step t∈T, the algorithm chooses it∈K and obtains a reward Rit,t. The reward Rit,t is a mapping [formula].

The generic adversarial bandit is detailed in Algorithm [\ref=nashelec:gsba]. In the case of adversarial problems, when we search for a Nash equilibrium for a reward function (k,s)  ↦  R(k,s), two bandit algorithms typically play against each other. One of them is Nature, and the other plays our role. At the end, our bandit algorithm recommends a (possibly mixed) strategy over the K arms. This recommended distribution is often the empirical distribution of play during the games against the Nature bandit.

Such a fast approximate solution can be provided by Exp3 (Exponential weights for Exploration and Exploitation) [\citep=auer2002finite] and its Exp3.P variant [\citep=auer2002nonstochastic], presented in Algorithm [\ref=nashelec:exp3p]. Exp3 has the same efficiency as the Grigoriadis and Khachiyan method [\citep=grigoriadis] for finding approximate Nash equilibria, and can be implemented with two bandits playing one against each other, e.g. one for us and one for Nature. Exp3.P is not anytime: it requires the time horizon in order to initialize some input meta-parameters. [\cite=busa2010fast] optimized Adaptive Boosting (AdaBoost), a popular machine-learning meta-algorithm, by the adversarial bandit algorithm Exp3.P, and proposed two parametrizations of the algorithm, as detailed in Table [\ref=nashelec:param]. [\cite=bubeck2012regret] proved a high probability bound on the weak reward of Exp3.P.

Another ingredient under the hood: sparsity

[\cite=teytaud2011upper] proposed a truncation technique on sparse problem. Considering the Nash equilibria for two-player finite-sum matrix games, if the Nash equilibrium of the problem is sparse, the small components of the solution can be removed and the remaining submatrix is solved exactly. This technique can be applied to some adversarial bandit algorithm such as Grigoriadis' algorithm [\citep=grigoriadis], Exp3 [\citep=auer2002finite] or Inf [\citep=bubeck2009pure]. The properties of this sparsity technique are as follows. Asymptotically in the computational budget, the convergence to the Nash equilibria is preserved [\citep=teytaud2011upper]. The computation time is lower if there exists a sparse solution [\citep=teytaud2014sparse]. The support of the obtained approximation has at most the same number of pure strategies and often far less [\citep=teytaud2011upper]. Essentially, we get rid of the random exploration part of the empirical distribution of play.

Overview of our method

We first give a high level view of our method, in Algorithm [\ref=nashelec:SNash]. All the algorithmic challenge is hidden in the tExp3.P algorithm, defined later. We now present the computation engine tExp3.P. We apply the truncation technique [\citep=teytaud2011upper] to Exp3.P. We present in Algorithm [\ref=nashelec:texp3p] the resulting algorithm, denoted as tExp3.P.

Experiments

We propose a simple model of investments in power systems. Our model is not supposed to be realistic; it is aimed at being easy to reproduce.

Power investment problem

We consider each investment policy, sometimes called action or decision, a vector [formula]. A scenario is a vector [formula]. Detailed descriptions of parameters are provided in Tables [\ref=nashelec:ds] and [\ref=nashelec:as].

Let S be the set of possible scenarios and K be the set of possible policies. The utility function R is a mapping [formula]. Given decision [formula] and scenario [formula], a reward can be computed by

[formula]

where cost is a meta-parameter. This provides a reward function [formula], with which we can build a matrix R of rewards. However, with a ternary discretization for each variable we get a huge matrix, that we will not construct explicitly - more precisely, it would be impossible to construct it explicitly with a real problem involving hours of computation for each [formula]. Fortunately, approximate algorithms can solve Nash equilibria with precision ε with O(K log (K) / ε2) requests to the reward function, i.e. far less than the quadratic computation time K2 needed for reading all entries in the matrix. We do experiments on this investment problem and apply the algorithms described in Table [\ref=nashelec:param]. We consider policies and scenarios in discrete domains: [formula],  [formula]. The reward matrix R310  ×  39 can be defined by [formula], but the reward is noisy as previously mentioned, where [formula] denotes the ith policy in K and [formula] denotes the jth scenario in S. Thus, each line of the matrix is a possible policy and each column is a scenario, Ri,j is the reward obtained by apply the policy [formula] to the scenario [formula]. Experiments are performed for different numbers of time steps in the bandit algorithms, i.e. we consider T simulations for each T∈{1,2,8,10,32,128,512,2048}  ×  ⌈310  /  10⌉. Thus when playing with the "theoretical" parametrization, for each T, the input meta-parameters η and γ are different, as they depend on the budget T. In the entire paper, when we show an expected reward [formula] for some [formula] and for [formula] learned by one of our methods, we refer to 10000 trials; [formula] are played for 10000 randomly drawn pairs [formula] i.i.d. according to the random variables in and jn proposed by the considered policies. The performance is the average reward of these 10000 trials [formula]. There is an additional averaging, over learning. Namely, each learning (i.e. the sequence of Exp3 iteration for approximating a Nash equilibrium) is repeated 100 times. The meta-parameter cost is set to 1 in our experiments.

We use the parametrizations of variants of Exp3.P presented in Table [\ref=nashelec:param]. [\cite=teytaud2011upper] proposed α = 0.7 as truncation parameter in truncated Exp3.P and [\cite=teytaud2014sparse] used the same value. The sparsity level, as well as the performance, are given in Table [\ref=nashelec:sparse]. We validate the good performance of α = 0.7. However, the sparsity is better with higher values - but these higher values do not always provide better results than the original non-sparse bandit.

We observe that when the number of simulations is bigger than the cardinality of the search domain, i.e. the number of possible pure policies, then α≃0.9 leads to better empirical mean reward against the uniform policy. Values between 0.5 and 1 are the best ones. When learning with few simulations (5905 = ⌈K  /  10⌉), the non-truncated solutions and non-sparse solutions are as weak as a random strategy. Along with the increment of simulation times, the non-truncated solutions and non-sparse solutions become stronger, but still weaker than the truncated solutions. When we use the truncation, we get significant mean reward even with a small horizon, i.e. the tExp3.P + t succeeds in finding better and "purer' policies than Exp.3.

The parameters of Exp3.P + t

When learning with few simulations (5905 = ⌈K  /  10⌉), the non-truncated solutions and non-sparse solutions are as weak as a random strategy. Along with the increment of simulation times, the non-truncated solutions and non-sparse solutions become stronger, but still weaker than the truncated solutions. Sparsity level "0.01" means that one and only one solution of the 100 learnings has one element above the threshold ζ, the other 99 solutions of the 99 learnings have no element above the threshold ζ. This situation is not far from the non-truncated or non-sparse case. If the solution is sparse, we get a better empirical mean reward even with a small horizon, i.e. the tExp3.P + t succeeds in finding better pure policies.

We see that truncated algorithms outperform their non-truncated counterparts, in particular, truncation clearly shows its strength when the number of simulations is small in front of the size of search domain.

A modified power investment problem

Now we modify the reward function as follows:

[formula]

where cost and c are meta-parameters.

As presented in the previous section, we can build a matrix R' with the reward function [formula]. We do experiments on this modified investment problem and apply the algorithms described in Table [\ref=nashelec:param]. We consider policies and scenarios in discrete domains as used in the previous section. The meta-parameters cost is set to 1 and c is set to [formula] in our experiments. The reward matrix is normalized in the experiments.

We present the results with c = 1 and c = 10 in Tables [\ref=nashelec:c1] and [\ref=nashelec:c10]: in both testcases, α = 0.9 does not provide good results when T = K, however α = 0.7 (recommended by previous works) is always better than the baseline, to which the truncation technique is not applied; for the testcase with c = 1, α = 0.9 outperforms the other values of α at most of time; when the budget is big, α = 0.99 provides better results.

Conclusion: Nash-methods have computational and modeling advantages for decision making under uncertainties

We propose a new criterion (based on Nash equilibria) and a new methodology (based on adversarial bandits + sparsity) for decision making with uncertainty. Technically speaking, we tuned a parameter-free adversarial bandit algorithm tExp3.P + t, for large scale problems, efficient in terms of performance itself, and also in terms of sparsity. tExp3.P + t performed better than tExp3.P without truncation. Moreover, tExp3.P + t with truncation parameter α = 0.7, which is theoretically guaranteed  [\citep=teytaud2011upper], got stable performance in the experiments.

From a user point of view, we propose a tool with the following advantages: (i) Natural extraction of interesting policies and critical scenarios. However, we point out that α = .7 provides stable (and proved) results, but the extracted submatrix becomes easily readable (small enough) with larger values of α. (ii) Faster computational cost than the Wald or Savage classical methodologies. Our methodology only requires a mapping R:(k,s)  ↦  R(k,s), which computes the outcome if we use the policy k and the outcome is the scenario s. Multiple objective functions can be handled: if we have two objectives (e.g. economy and greenhouse gas pollution), we can just duplicate the scenarios, one for which the criterion is economy, and one for which the criterion is greenhouse gas. Given a problem, the algorithm will display a matrix of rewards for different policies and for several scenarios (including, by the trick above, several criteria such as particular matter, greenhouse, and cost).

As a summary, we get a fast criterion, faster than Wald's or Savage's criteria, with a natural interpretation. The algorithm naturally provides a matrix of results, namely the matrix of outcomes in the most interesting decisions and for the most critical scenarios. These decisions and scenarios are also equipped with a ranking.