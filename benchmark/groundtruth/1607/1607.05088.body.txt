Towards Personality-Aware Recommendation

Introduction

Nowadays, online shopping plays an increasingly significant role in our daily lives [\cite=Forrester]. Most consumers shop online with the majority of these shoppers preferring to shop online for reasons like saving time and avoiding crowds. Marketing campaigns can create awareness that drive consumers all the way through the process to actually making a purchase online [\cite=Kim:2011]. Accordingly, a challenging problem is to provide the user with a list of recommended advertisements they might prefer, or predict how much they might prefer the content of each advert.

Attitudes, perceptions and motivations are not apparent from clicks on advertisements or online purchases, but they are an important part of the success or failure of online marketing strategies. A person's buying choices are further influenced by psychological factors like impulsiveness (e.g., leads to impulse buying behaviors), neuroticism, extraversion which affect their motivations and attitudes [\cite=Turkyilmaz201598]. However, the impact of personality factors on advertisements has been studied at the level of social sciences and microeconomics [\cite=Bosnjak2007] [\cite=Turkyilmaz201598] [\cite=soBob], but largely neglected, to the best of our knowledge, at the level of advert recommender systems. Taking the lack of publicly available benchmark which include questionnaires consisting of the items of validated scales (e.g., Five Factor Model [\cite=rammstedt:2007]) on the involved individuals, this study is meant to contribute to this unexplored area.

This paper presents the ADS Dataset, a highly innovative collection of 300 real advertisements rated by 120 individuals and enriched with their personality traits. The user study is conducted by recruiting a set of test subjects, and asking them to perform several tasks. The experimental protocol adopted for the data collection has been designed to capture users' preferences in a controlled usage scenario so as to avoid any biases. Moreover, this work carries out experiments with various evaluation criteria used in the literature and provides a library within one integrated toolbox.

Summarizing, the contribution of this work is three-fold:

Dataset: we collect a representative benchmark for computational advertising enriched with affective-like features such as personality factors and visual cues from users' favorite pictures. The benchmark allows us to (i) explore the relationship between consumer characteristics, attitude toward online shopping and advert recommendation, (ii) identify the underlying dimensions of consumer shopping motivations and attitudes toward online in-store conversions, and (iii) have a reference benchmark for comparison of state-of-the-art advertisement recommender systems (ARSs).

Code library: we integrated most widely used evaluation metrics in the proposed code library with uniform input and output formats to facilitate large scale performance evaluation.

Evaluation metrics: we collect two broad classes of prediction accuracy measures, depending on the task the recommender system is performing: "ad rating prediction" or "ad click prediction".

The code library, annotated dataset and all the results are available on project page.

The rest of the paper is organized as follows: in Section [\ref=sec:dataset] we present and describe the proposed ADS Dataset. In Section [\ref=sec:Algorithms] we show the baseline techniques used for the recommendation of advertisements. Then, in Section [\ref=sec:eval], we survey a large set of evaluation metrics in the context of the property that ARSs evaluate. In Section [\ref=sec:results] results are reported for each one of the two scenarios taken into account in this work, and finally, in Section [\ref=sec:conclusion] conclusions are given, and future perspectives are envisaged.

ADS Dataset

The corpus includes 300 advertisements voted by unacquainted individuals (120 subjects in total. Note, the data collection process is still running). Advert content is categorized in terms of the 20 main Amazon product categories. Adverts equally cover three display formats: Rich Media Ads, Image Ads, Text Ads (i.e., 100 ads for each format). Participants rated (from 1-star to 5-stars) each recommended advertisement according to if they would or would not click on it. Inspired from recent findings which investigate the effects of personality traits on online impulse buying [\cite=Bosnjak2007] [\cite=Turkyilmaz201598] [\cite=soBob], and many other approaches based upon behavioral economics, lifestyle analysis, and merchandising effects [\cite=Bosnjak2007] [\cite=Mowen2000], this study takes a trait theory approach to studying the effect of personality on user's motivations and attitudes toward online in-store conversions. Therefore, the corpus includes the Big Five Inventory-10 (BFI-10) to measure personality traits [\cite=rammstedt:2007], which allows the exploration of this promising research direction.

Recent soft-biometric approaches have shown the ability to infer the personality traits of users from visual cues extracted from their favorite pictures (e.g., in Computational Aesthetics), or from users' writing behaviors in social media settings [\cite=Roffo:icmi2014] [\cite=Roffo:HBU2014] [\cite=Roffo_2013_ICCV_Workshops]. While not necessarily corresponding to the actual traits of an individual, attributed traits are still important because they are predictive of important aspects of social life, including attitude of others and social identity. As a result, the proposed benchmark also includes 1,200 spontaneously uploaded images that hold a lot of meaning for the participants and their related annotations: positive/negative (see Table [\ref=tab:features] for further details).

The Subjects

This corpus involves 120 English native speakers between 18 and 68. The median of the participants age is 28. Most of the participants have a university education, 31% of them are undergraduate students. In terms of gender, 77 are females and 43 males. The percentage distribution of household income within the sample is: 23% less or equal to 11K USD per year, 48% from 11K to 50K USD, 21% from 50K to 85K USD, and 8% more than 85K USD. The median income is between 11K and 50K USD.

Evaluated Algorithms

Since a prediction engine lies at the basis of the most recommender systems, we selected some of the most widely used techniques for recommendations and predictions [\cite=He:2014], such as Logistic Regression (LR) [\cite=LIBLINEAR], Support Vector Regression with radial basis function (SVR-rbf) [\cite=Chang:2011], and L2-regularized L2-loss Support Vector Regression (L2-SVR) [\cite=LIBLINEAR]. These engines may predict user opinions to adverts (e.g., a user's positive or negative feedback to an ad) or the probability that a user clicks or performs a conversion (e.g., an in-store purchase) when they see an ad. In Section [\ref=sec:results], we evaluate these methods while feeding them with and without features coming from the psychometric traits.

Evaluation Methodology

Research in the ARS field requires quality measures and evaluation metrics to know the quality of the techniques, methods, and algorithms for predictions and recommendations. In this section we review the process of evaluating an ARS on two main tasks: (i) measuring the accuracy of rating predictions, and (ii) measuring the accuracy of click predictions.

Ad Rating Prediction

In this first scenario, we want to predict the feedback a user would give to an advert (e.g. 1-star through 5-stars). In such a case, we want to measure the accuracy of the system's predicted ratings. Root Mean Squared Error (RMSE) is perhaps the most popular metric used in evaluating the accuracy of predicted ratings. The system generates predicted ratings r̂u,a for a test set T of user-advert pairs (u,a) for which the true ratings ru,a are known. The RMSE between the predicted and actual ratings is given by:

[formula]

Mean square error (MSE) is an alternative version of RMSE, the main difference between these two estimators is simply that MSE has the same units of measurement as the square of the quantity being estimated, while RMSE has the same units as the quantity being estimated.

Mean Absolute Error (MAE) is a popular alternative, given by

[formula]

Ad Click Prediction

In many applications the recommendation system tries to recommend adverts to users in which they may be interested. For example, when items are added to the queue, Amazon suggests a set of adverts on which the user would most probably click. In this case, we are not interested in whether the system properly predicts the ratings of these adverts but rather whether the system properly predicts that the user will click on them (e.g. they perform a conversion). Therefore, we then have four possible outcomes for a recommended advertisement, as shown in Table  [\ref=tab:table1].

We can count the number of examples that fall into each cell in the table and compute the following quantities: Precision = [formula], Recall (True Positive Rate) = [formula], and False Positive Rate (1 - Specificity) = [formula].

We can expect a trade-off between these quantities; while allowing longer recommendation lists typically improves recall, it is also likely to reduce the precision. We can compute curves comparing precision to recall, or true positive rate to false positive rate. Curves of the former type are known simply as precision-recall curves, while those of the latter type are known as a Receiver Operating Characteristic or ROC curves. A widely used measurement that summarizes the precision recall of ROC curve is the Area Under the ROC Curve (AUC) [\cite=BAMBER1975] which is useful for comparing algorithms independently of application.

Prediction Experiments

In this section we show results obtained for the two types of scenarios introduced in Sec. [\ref=sec:eval]. We conducted rigorous experiments to explore the strengths and weakness of the proposed algorithms when taking into account personality traits as features.

Let us say [formula] is the set of observations, where the vectors [formula] correspond to features coming only from the group "users' preferences" as described in Table [\ref=tab:features], and N  =  120 stands for the number of users involved in the experiment. Regression is performed over the 20 product categories. Given the user ui, labels are assigned to each category by averaging the votes they gave to the category items such as ui  =  {l1,...,l20},l∈[1  -  5]. The prediction problem is solved using LR, L2-SVR, and SVR-rbf, while feeding them with and without features coming from "personality traits". All experiments were performed using a k-fold approach (k = 10) and the folds are the same for every algorithm in comparison. In k-fold cross-validation, X is randomly partitioned into k's equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k - 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used only once as the validation data. The k results from the folds can then be averaged to produce a single estimation.

The representation above serves as a basis for the feature ranking and selection strategy. Ranking features allow us to detect a subset of cues which is relevant and not redundant. Accordingly, we use the training data obtained after the split as input of the infinite feature selection (Inf-FS) [\cite=Roffo:InfFS:2015] algorithm. By construction, the Inf-FS is a graph-based method which exploits the convergence properties of the power series of matrices to evaluate the relevance of a feature with respect to all the other ones taken together. Indeed, in the Inf-FS formulation, each feature is mapped on an affinity graph, where nodes represent features, and weighted edges the relationships between them. In particular, the graph is weighted according to a function which takes into account both correlations and standard deviations between feature distributions. Each path of a certain length l over the graph is seen as a possible selection of features. Therefore, varying these paths and letting them tend to an infinite number permits the investigation of the importance of each possible subset of features. The Inf-FS assigns a final score to each feature of the initial set; where the score is related to how much the given feature is a good candidate regarding the regression task. Therefore, ranking the outcome of the Inf-FS in descendant order allows us to perform the subset feature selection throughout a model selection stage. In this way, we reduce the number of features, by selecting 75% of the total. The selected features are: the number of favorite websites, T.V. programmes, sports, past times, the most watched movies and most visited websites, where we add the big-five personality traits.

Exp.1 Ad Rating Prediction

In this section we report results for rating prediction showing that traces of user's personality can improve the prediction performance of the evaluated methods significantly. Statistical evaluation of experimental results has been considered an essential part of validation of machine learning methods.

Figure [\ref=Figure:1] shows prediction results in term of RMSE, MSE and MAE. This first analysis shows how personality traits affect prediction performance. To this end, t-tests have been used for comparing prediction accuracies, showing a statistical significant effect of personality traits while using L2-SVR (p-value < 0.05) and LR (p-value < 0.01).

Exp.2 Ad Click Prediction

This section shows an offline evaluation of click prediction. Along the lines of the previous experiment, a k-fold cross-validation is used. The prediction is performed by category, whenever a user showed their interest (clicks) on the majority of the items of a given category (50% + 1) we labeled the category as "clicked", otherwise "not clicked". Therefore, given the methods in comparison, we compute precision-recall and ROC curves for each fold and for each category, and then we average the resulting curves over folds and categories. Figure [\ref=Figure:2].(a) reports the precision-recall curves which emphasize the proportion of recommended items that are preferred and recommended. Figure [\ref=Figure:2].(b) shows the global ROC curves for LR and LR-B5, which emphasize the proportion of adverts that are not clicked but end up being recommended. The LR-B5 curve completely dominates the other curve, the decision about the superior setting for LR is easy. The Area Under the ROC Curve is calculated as a measure of accuracy, which summarizes the precision recall of ROC curves, we report AUC, precision and recall for all the methods in Table [\ref=tab:table2].

Conclusions

In this paper, we presented the ADS Dataset, a collection of 300 real advertisements rated by 120 unacquainted individuals. The corpus has been collected with the main goal of studying the possible achievable benefits of employing personality traits in modern recommender systems. To obtain stronger and more relevant results for this community, appropriate and high-level features needed to be designed that carry important information for inference. In this paper, we only use raw data and mainly focus on the standard techniques used for recommendation of items in order to set a baseline for future work. We reviewed a large set of properties, and explain how to evaluate systems given relevant properties. We then discuss how to compare ARS based on a set of properties that are relevant for the application. Therefore, we review two main types of experiments in an off-line setting, where recommendation approaches are compared with different selections of features (i.e., with and without personality traits) accordingly with our goal.