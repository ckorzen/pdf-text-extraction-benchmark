Intoduction

Multi-relational data, which refers to directed graphs whose nodes correspond to entities and edges represent relations that link these entities, plays a pivotal role in many areas such as recommender systems, the Semantic Web, or computational biology. Relations are modeled as triplets of the form (head, label, tail), where label indicates the type of link between the entities head and tail. Relations are thus of several types and can exhibit various properties (symmetry, transitivity, irreflexivity, etc.). Such graphs are popular tools for encoding data via knowledge bases (KBs), semantic networks or any kind of database following the Resource Description Framework format. Hence, they are widely used in the Semantic Web (e.g. Freebase or Google Knowledge Graph but also for knowledge management in bioinformatics (e.g. GeneOntology) or natural language processing (e.g. WordNet).

Despite their appealing ability for representing complex data, multi-relational databases remain complicated to manipulate because of the heterogeneity of the relations (frequencies, connectivity), their inherent noise (collaborative or semi-automatic creation) and their very large dimension (up to millions of entities and thousands of relation types).

In this paper, we introduce a distributed model, which learns to embed such data in a vector space, where entities are modeled as low-dimensional embeddings. Many existing approaches (e.g. from [\citet=Sutskever:2009] [\citet=Nickel:2011]) interpret relations as linear transformations of these embeddings: when [formula] holds, then the embeddings of head h and tail t should be close (in the embedding space) after transformation by a linear operator that depends on the label [formula]. With such an interpretation, the model implies that the relation is reflexive since the embedding of h will always be its nearest neighbor, and because of the triangle inequality, the model will, to some extent, imply some form of transitivity of the relation.

While this interpretation is fine for equivalence relations (such as WordNet's _similar_to), it is inadequate for irreflexive relations that represent hierarchies, such as WordNet's _hypernym or Freebase's type hierarchy. Indeed, taking the simplest example of entities organized in a tree with two relations, "sibling" and "parent", the embeddings of siblings should be close to each other (since it essentially is an equivalence relation), but enforcing the constraint that parent nodes should be close to their child nodes will lead the embedding of the whole tree to collapse to a small region of the space where the siblings and parent of a given node are impossible to distinguish.

Since hierarchical and irreflexive relations are extremely common in KBs, we propose a simple model to efficiently represent them, by interpreting relations as translations in the embedding space: if [formula] holds, then the embedding of t should be close to the embedding of h plus some vector that depends on [formula]. This approach is motivated by the natural representation of trees (i.e. embeddings of the nodes in dimension 2): while siblings are close to each other and nodes at a given height are organized on the x-axis, the parent-child relation corresponds to a translation on the y-axis. Another, secondary, motivation comes from the recent work of [\citet=DBLP:journals/corr/abs-1301-3781], in which the authors learn word embeddings from free text, and some one-to-one relations between entities of different types, such as the relation "capital of" between countries and cities, are (coincidentally rather than willingly) represented by the model as translations in the embedding space. Our approach may then be used in the context of learning word embeddings in the future to reinforce this kind a structure of the embedding space.

Apart from the main line of algorithms to learn embeddings of KBs, a number of recent approaches deal with the asymmetry of the relations at the expense of an explosion of model parameters. We present an empirical evaluation on data dumps of WordNet and Freebase, in which our model achieves strong results compared to such algorithms, with much fewer parameters and even lower dimensional embeddings.

In the remainder of the paper, we describe some of the related work in Section [\ref=sec:rwork]. We then describe our model in Section [\ref=sec:model], and discuss its connections with related methods. We report preliminary experimental results on WordNet and Freebase in Section [\ref=sec:exp]. We finally sketch some future work directions in Section [\ref=sec:concl].

Related work

Most previous methods designed to model relations in multi-relational data rely on latent representations or embeddings. The simplest form of latent attribute that can be associated to an entity is a latent class. Several clustering approaches have been proposed. [\citet=Kemp:2006] considered a non-parametric Bayesian extension of the stochastic blockmodel allowing to automatically infer the number of latent clusters; [\citet=Kok:2007] introduced clustering in Markov-Logic networks; [\citet=Sutskever:2009] used a non-parametric Bayesian clustering of entities embedding in a collective matrix factorization formulation. All these models cluster not only entities but relation labels as well.

These methods can provide interpretations and analysis of the data but are slow and do not scale to large databases, due to the high cost of inference. In terms of scalability, models based on tensor factorization (like those from [\citep=Harshman:1994] or [\cite=Nickel:2011]) have shown to be efficient. However, they have been outperformed by energy-based models [\cite=bordesAAAI11] [\cite=jenatton2012latent] [\cite=bordesMLJ2013] [\cite=chen2013]. These methods represent entities as low-dimensional embeddings and relations as linear or bilinear operators on them and are trained via an online process, which allows them to scale well to large numbers of entities and relation types. In Section [\ref=sec:exp], we compare our new approach to SE [\cite=bordesAAAI11] and SME [\cite=bordesMLJ2013].

Translation-based model

We now describe our model and discuss its relationship to existing approaches.

Our model

Given a training set S of labeled arcs [formula], our goal is to learn vector embeddings for all values of h, [formula] and t. We assume all nodes and labels appear at least once in the training set. The embeddings take values in [formula] (k is a model hyperparameter) and are denoted with the same letter, in boldface characters. The basic idea behind our model is that the functional relation induced by the [formula]-labeled arcs corresponds to a translation of the embeddings, i.e. we want that [formula] when [formula] holds, while [formula] should be far away from [formula] otherwise.

To learn such embeddings, we minimize the following margin-based ranking criterion over the training set:

[formula]

where + denotes the positive part of x, γ > 0 is a margin hyperparameter, [formula] is some dissimilarity function on [formula], e.g. the euclidian distance or the squared euclidian distance, and

[formula]

The set of "negative" examples we sample according to Equation [\ref=eq:negativeExs] is basically the training ("positive") triple with either the head or tail replaced by a random entity (but not both at the same time). The loss function [\eqref=eq:objectiveFunc] favors low values of dissimilarity between head+label and tail for positive triplets, and large values for negative triplets, and is thus a natural implementation of the intended criterion.

The minimization is carried out by stochastic gradient descent, over the possible [formula] and [formula], with the additional constraints that the L2-norm of the embeddings of the entities is 1 (no regularization or norm constraints are given to the label embeddings [formula]).

Relationship to previous approaches

Section [\ref=sec:rwork] described a large body of work on embedding KBs. We detail here the relationships between our model and those of [\citet=bordesAAAI11] (Structured Embeddings or SE) and [\citet=chen2013].

SE [\citep=bordesAAAI11] embeds nodes into [formula], and labels into two matrices [formula] and [formula] such that [formula] is small for positive triplets [formula] (and large otherwise). The basic idea is that when two nodes belong to the same edge, their embeddings should be close to each other in some subspace that depends on the label. This basic idea would imply [formula], and using two different projection matrices for the head and for the tail is intended to account for the possible asymmetry of relation [formula]. When the dissimilarity function takes the form of [formula] for some [formula] (e.g. g is a norm), then the model of SE with an embedding of size k + 1 is strictly more expressive than our model with an embedding of size k, since linear operators in dimension k + 1 can reproduce affine transformations in a subspace of dimension k (by constraining the k + 1st dimension of all node embeddings to be equal to 1). SE, with [formula] as the identity matrix and [formula] taken so as to reproduce a translation is then equivalent to our model. Despite the lower expressiveness of our model, we still reach better performance than this model in our experiments because (1) our model is a more direct way to represent the true properties of the relations, and (2) regularization, and more generally any form of capacity control, is difficult in embedding models ; greater expressiveness may then be more synonymous to overfitting than to better performance.

Another related model is the Neural Tensor Model of [\citet=chen2013]. A special case of that model (which would actually boil down to a "Neural Matrix Model") corresponds to learn scores [formula] (higher scores for positive triplets) of the form:

[formula]

where [formula], [formula] and [formula], all of them depending on [formula].

If we consider our model with the squared distance as dissimilarity function, we have: Considering our norm constraints ([formula]) and the ranking criterion [\eqref=eq:objectiveFunc], in which [formula] does not play any role in comparing positive and negatives triplets, our model thus corresponds to the scoring triplets according to [formula], and thus corresponds to [\citet=chen2013]'s model (Equation [\eqref=eq:chenmodel]) where [formula] is the identity matrix, and [formula]. We could not run experiments with that model, but once again our model has much fewer parameters: this should ease the training and prevent overfitting, and hence compensate for a lower expressiveness.

Experiments

Our approach is evaluated against the methods SE and SME (Semantic Matching Energy) from [\citep=bordesAAAI11] [\citep=bordesMLJ2013] on two data sets and using the same ranking setting for evaluation.

We measure the mean and median predicted ranks and the top-10, computed with the following procedure. For each test triplet, the head is removed and replaced by each of the entities of the dictionary in turn. Energies (i.e. dissimilarities) of those corrupted triplets are computed by the model and sorted by ascending order and the rank of the correct entity is stored. This whole procedure is also repeated when removing the tail instead or the head. We report the mean and median of those predicted ranks and the top-10, which is the proportion of correct entities in the top 10 ranks.

Data

We used data from two KBs; their statistics are given in Table [\ref=tab:data].

WordNet

This knowledge base is designed to produce an intuitively usable dictionary and thesaurus, and support automatic text analysis. Its entities (termed synsets) correspond to word senses, and relation types define lexical relations between them. We considered the data version used in [\citep=bordesMLJ2013]. Examples of triplets are ( _score_NN_1, _hypernym, _evaluation_NN_1) or ( _score_NN_2, _has_part, _musical_notation_NN_1).

Freebase

Freebase is a huge and growing database of general facts; there are currently around 1.2 billion triplets. To make a small data set to experiment on we selected the subset of entities that are also present in the Wikilinks database and that also have at least 100 mentions in Freebase (for both entities and relations). We also removed negative relations like '!/people/person/nationality' which just reverses the head and tail compared to the relation '/people/person/nationality'. This resulted in 592,213 triplets with 14,951 entities and 1,345 relations which were randomized and split as shown in Table [\ref=tab:data].

Implementation

We implemented our model using the SME library, which already proposes code for SE and SME. The dissimilarity measure d was set to the L1 distance, mostly because it led to a faster training.

For this preliminary set of experiments, we did not perform an extensive search for hyperparameters. For experiments of our method on WordNet, we fixed the learning rate for the stochastic gradient descent to 0.01, the dimension k of the embeddings to 20 and chosen the margin γ among {1,2,10} with the validation set (optimal value was 2). We report results for SE and SME extracted from [\citep=bordesMLJ2013] where those models have been trained using a much more thorough hyperparameter search. For experiments on Freebase, we ran all experiments using the SME library with fixed values for the learning rate (= 0.01), k (= 50) and γ (= 1). For both datasets, the training time was limited to at most 1,000 epochs over the training set. The best model was selected using the mean predicted rank on the validation set.

Results

Tables [\ref=tab:wn] and [\ref=tab:fb] displays the results on both data sets for our method, compared to SE, to two versions of SME and to Unstructured, a simple model which only uses the dot-product between [formula] and [formula] as dissimilarity measure for a triplet [formula], with no influence of [formula]. Table [\ref=tab:fb_ex] gives examples of nearest link prediction results of our approach on the Freebase test set.

Our method greatly outperforms all counterparts on all metrics, with particularly good results for the top-10 metric. We believe that such remarkable performance is due to an appropriate design of the model according to the data, but also to its relative simplicity. Hence, even if the problem is non-convex, it can be optimized efficiently with stochastic gradient. We showed in Section [\ref=sec:pap] that SE is more expressive than our proposal. However, its complexity makes it quite hard to train as shown in the results of tables [\ref=tab:wn] and [\ref=tab:fb].

Table [\ref=tab:fb_ex] illustrates the capabilities of our model. Given a head and a label, the top predicted tails (and the true one) are depicted. The examples come from the Freebase test set. Even if the good answer is not always top-ranked, the predictions reflect common-sense.

Conclusion and future work

We proposed a new approach to learn embeddings of KBs, focusing on the minimal parametrization of the model to accurately represent hierarchical and irreflexive relations. This short paper is essentially intended to be a proof-of-concept that translations are adequate to model such relations in a multi-relational setting. It can be improved and better validated in several ways. For the experimental evaluation, this paper is the first one to present link prediction on this dump of Freebase. More benchmarking is needed, such as the comparison with models of [\citet=chen2013] and [\citet=jenatton2012latent]. We also intend to consider learning translations of word embedding, either from free text as in [\cite=DBLP:journals/corr/abs-1301-3781] or from (subject,verb,object) triplets as in [\cite=bordesAAAI11].

Finally, regarding modeling relations, equivalence relations in our approach are represented by a [formula] translation vector, and thus enforces all members of an equivalence class to be close to each other in the embedding space (whatever the relation). Some additional degrees of freedom may be given by adding a projection matrix to each relation, so that equivalence relations only enforce entities to be close to each other in some subspace of the embedding space. However, this would increase the number of parameters, and we believe that regularization and optimization techniques should be further studied to achieve optimal performance.

Acknowledgments

We thank Thomas Strohmann and Kevin Murphy for useful discussions. This work was supported by the French ANR (EVEREST-12-JS02-005-01).