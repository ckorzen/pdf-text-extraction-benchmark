Coupled Depth Learning

Introduction

Over the last few years depth estimation has been the subject of active research by the machine learning and computer vision community [\cite=BaigJPBDS14] [\cite=Karsch:TPAMI:14] [\cite=konrad2013learning] [\cite=Liu_2014_CVPR] [\cite=Ladicky_2014_CVPR]. This can partly be attributed to the fact that algorithms using the depth channel as an additional cue have shown dramatic improvements over their RGB counterparts on a number of challenging vision problems [\cite=ZhangICCV2013] [\cite=guptaECCV14] [\cite=Hen12RGB] [\cite=Hermans14ICRA]. Most of these improvements have been demonstrated using depth measured by hardware sensors. However, most of the pictures available today are still traditional RGB (rather than RGBD) photos. Thus, there is a need to have robust algorithms for estimating depth from single RGB images. While inferring depth from a single view is ill-posed in general (an infinite number of 3D geometric interpretations can fit perfectly well any given photo), physical constraints and statistical regularities can be exploited to learn to predict depth from an input photo with good overall accuracy. In this work we propose to learn these spatial and statistical regularities from a RGBD training set in the form a global depth basis. We hypothesize that the depth map of any image can be well approximated by a linear combination of this global depth basis. Following this reasoning we formulate coarse depth estimation as the problem of predicting the coefficients of the linear combination from the input image. Our design choice makes this regression problem easier as the target dimensionality is much lower than the number of pixels and the output space is more structured. Crucially, we learn the depth basis and the regression model jointly by optimizing a single learning objective.

As input for our regression problem we use a holistic image representation capturing the coarse spatial layout of the scene. While in principle we could attempt to learn this holistic feature descriptor too, we argue that existing RGBD repositories are too limited in scope and size to be able to learn features that would generalize well to different datasets. Instead, we propose to leverage a pretrained global feature representation that has been optimized for scene classification using a collection of 2.5 million examples [\cite=zhou2014places]. The intuition is that since these features have been tuned to capture spatial and appearance details that are useful to discriminate among a large number of scene categories, we expect them to be also effective generic features for the task of depth prediction. Our experiments on two highly distinct benchmarks validate this hypothesis as our models trained on these scene features yields state-of-the-art results (without any fine-tuning).

Since our model is trained on a holistic description of the image, it can be argued that it is implicitly optimized to predict the main global 3D structure in the scene, possibly at the expense of fine depth details. To address this potential shortcoming we propose a local refinement step, which uses the global estimate to improve the depth prediction at individual pixels. This is achieved by training a depth refinement function on hypercolumn features of individual pixels [\cite=BharathCVPR2015], which describe the local appearance and context in the neighborhood of the pixel. Our experiments indicate that the local refinement quantitatively improves the global estimate and produces finer qualitative details. In Table [\ref=tb:visResultsNYUV2] we show the global and locally-refined depth outputs produced by our system for a few example images.

Related Work

While initial approaches to depth estimation exploited specific cues like shading [\cite=Zhang99shapefrom] and geometry [\cite=Hedau_2009_SpatialLayout], more recently the focus has shifted toward employing pure machine learning methods due to the heavily restrictive assumptions of these earlier methods. Most of the earlier machine learning based approaches [\cite=saxena2005learningdepth] [\cite=saxena2009make3d] [\cite=LiuCVPR2010semanticdepth] operate in a bottom-up fashion by performing local prediction (e.g., estimating depth for individual patches or superpixels) and by spatially smoothing these estimates with a CRF or a MRF model. The advantage of local prediction models is that they can be trained well even with limited RGBD data since they treat each patch or pixel in the collection as a separate example. However, small regions do not capture enough context for robust depth estimation. In contrast, we approach depth estimation first at a global level by considering the entire image at once. Then we regress depth at a per-pixel level using the global estimate as a prior.

Recently, there has been an increased interest in nonparametric models [\cite=Karsch:TPAMI:14] [\cite=konrad2013learning]. These approaches find nearest-neighbors of the query in the training set, and then fuse the depth maps of the retrieved neighbors to produce a depth estimate for the query. Thus, they require all of the data to be stored on disk at inference time in order to perform the database search, which is costly in terms of both computation and storage.

Recently, there has been an increased interest in applying deep learning methods[\cite=EigenPF14] [\cite=Wang_2015_CVPR] [\cite=CVPR15bFayao] for estimating depth from a single image. [\cite=EigenPF14] [\cite=Wang_2015_CVPR] attempt to regress depth directly from the image. This means they need hundreds of thousands of examples train their models to avoid overfitting. This means that these approaches cannot be used for depth estimation in areas where data is scarce. [\cite=CVPR15bFayao] attempts to learn deep features for predicting super-pixel depths directly and then try to enforce coherence with a CRF to capture scene structure. Whereas this approach does work for smaller dataset, it is restricted to super-pixel level predictions and each super-pixel is facing the camera(has no gradient).

Our approach critically differs from these prior work in two fundamental aspects. First, our approach regresses on a small set of depth reconstruction weights rather than the full depth maps as is done by [\cite=EigenPF14] [\cite=Wang_2015_CVPR]. Our design choice exploits statistical regularities in the problem and reduces the number of outputs to predict. We demonstrate that this allows our method to achieve a much lower RMSE error than [\cite=EigenPF14] [\cite=Wang_2015_CVPR] [\cite=CVPR15bFayao] even when using 150 times less training data (on NYUv2). Second, our refinement model is trained to predict the depth at individual pixels using local pixel descriptors rather than super-pixels as is done by [\cite=CVPR15bFayao]. Furthermore, we also show how to leverage features from deep networks trained on relevant tasks to achieve much better performance.

Our joint optimization of depth basis and regression is inspired by the semi-coupled dictionary learning scheme described in [\cite=wang2012semi]. Vondrick et al. [\cite=Vondrick:iccv2013] also adapted this framework to visualize object detection features. In our work we borrow this optimization scheme to perform joint learning of a depth dictionary and a regressor from the image space to the dictionary weights. To the best of our knowledge this is the first application of couple dictionary learning and regression to depth estimation.

Technical Approach

In the following sub-sections we discuss how to jointly learn a global depth basis and a transformation for a given holistic image representation by using the training data. Let [formula] be the training set used to learn our model, where [formula] represents the i-th image (consisting of R rows, C columns and 3 color channels) and [formula] is its associated ground-truth depth map. We also show how to use the trained model for inferring coarse global depth. We further show how to use these coarse estimates to train and apply a refinement model to produce an estimate of Di which was our original goal.

Global Depth Estimation

Learning the Global Depth Model

To learn the global depth model, we start by downsampling the ground truth training depth maps. This has the effect of removing fine depth details (object boundaries, fine gradients denoting local shape, etc). We denote with [formula] the vector obtained by vectorizing the resized depth map Di, where PL represents the low resolution depth map dimensionality. Similarly, we indicate with [formula] the vector obtained by stacking the pixel values of the image one on top of the other. Our objective is to train a model that given an input image [formula] (at full resolution) predicts its global depth map [formula].

The first assumption we make is that the global depth map [formula] can be expressed as a linear combination of basis vectors from a depth basis [formula]:

[formula]

where the [formula] are the basis atoms and [formula] is the vector containing the image-specific mixing coefficients (or weights). Fig. [\ref=fig:varyingDictionary] shows both quantitatively as well as qualitatively the effect of varying the dictionary size on depth reconstruction. We propose to learn a mapping [formula] that predicts the depth reconstructive weights [formula] from the input image [formula]. Note that in our work m <  < PL (e.g., m = 48 for NYUv2 and m = 96 for KITTI) and thus the use of the basis to represents depth greatly reduces the number of outputs that the regression model needs to predict. To regress on [formula] we choose a simple kernel-based regression model

[formula]

where [formula] and [formula] is a vector containing radial basis functions φj() computed with respect to centers j for [formula]. The centers [formula] are example images (different from those included in the training set D) and selected according to the details described in section [\ref=implementationdetails]. Intuitively, they represent n prototypical scene images that allow us to express [formula] as a linear combination of kernel distances from [formula]. We compute the radial basis functions in terms of feature descriptors f(),f() extracted from the images ,. We use as image representation f() the features computed by layer "pool5" of the deep network of the PLACES model [\cite=zhou2014places]. This is the max-pooled output of the networks fifth and final convolutional layer. The feature map has dimensionality 6  ×  6  ×  256  =  9216. While prior work [\cite=Girshick:CVPR14] [\cite=Panda] [\cite=Karayev] has shown that the subsequent (fully connected) layers of the Krizhevsky [\cite=Krizhevsky] network (same architecture, different dataset) produce higher level representations that yield improved recognition accuracy, pool5 is the most appropriate feature map to use in our setting since it is the last layer preserving explicit location information before the "spatial scrambling" of the fully connected layers (note that a spatially variant representation is crucially necessary to predict the depth at each pixel). We validated experimentally this intuition and observed that using the feature maps from the fully-connected layers produced poorer depth prediction accuracy. Using this representation for feature vector f(), we then compute φj()  =   exp ( - ||f()  -  f(j)||2  /  2σ2j).

Given this model, a naïve approach to training our depth estimator is to learn disjointly the depth basis and the regression mapping. This would involve first learning the basis B and the weights [formula] of Eq. [\ref=eq:dict] (e.g., by minimizing the reconstruction error on training depths [formula]) and then regressing on these learned weights to estimate the transformation T of Eq. [\ref=eq:regr]. While straightforward, in our experiments we demonstrate that this two-step process yields much inferior results compared to a joint optimization over B, [formula], and T using a single learning objective that couples all of the parameters together. We refer to this learning objective as J(B,,T) and define it as follows:

[formula]

The first two terms of J encourage reconstruction of the depth maps using sparse weights and are equivalent to those in the traditional sparse coding objective [\cite=Lee_etal07:sparseCoding]. The third term imposes the requirement that the depth weights be "predictable" from the image input. The final term is a regularizer over the transformation T. Thus, joint optimization of J over all parameters will yield a depth basis B, depth weights [formula], and transformation T that simultaneously minimize 1) sparse reconstruction of depths maps and 2) regression error from the image domain to the depth space, subject to appropriate regularizations. In practice we minimize J(B,,T) subject to the constraints ||j||2  ≤  1 for [formula] in order to avoid scale degeneracies on B. Furthermore we enforce positivity constraints on the sparse weights wij in order to define a purely additive model of depth estimation. We have verified experimentally that this yields slightly better results than leaving the weights unconstrained. We also considered using an L2 sparsity over the weights i with but found that this yielded consistently slightly worse results, as also reported in prior articles [\cite=ng2004feature].

While our learning objective is not jointly convex over ,B,T, it is convex for each of these individual parameters when we keep the other two fixed. Based on this, we optimize our learning objective via block-coordinate descent by minimizing in turn with respect to 1) the basis, 2) the depth weights and 3) the transformation. These three alternating steps are considered in detail below:

Estimate weights [formula] given parameters B, T. It is easy to verify that minimizing J with respect to [formula] while keeping B, and T fixed (at the current estimate) reduces to a problem of the form:

[formula]

where i,C are constants written in terms of B and T. We solve this problem via least angle regression (LARS) [\cite=Efron04leastangle].

Learning the depth basis B given [formula], T. This amounts to a L2-constrained least-squares problem, which we solve using the Lagrange dual, as in [\cite=Lee_etal07:sparseCoding].

Learning the transformation T given [formula], B. This reduces to a L2-regularized least-squares problem, which can be solved in closed-form as shown in [\cite=wang2012semi].

We initialize this optimization by setting B and [formula] to the solution computed via sparse coding [\cite=Lee_etal07:sparseCoding], thus neglecting the terms in J depending on transformation T. We then compute T by solving step 3 above. Fig. [\ref=fig:coarsedict] shows the bases learned with this procedure on NYUv2 and KITTI.

Global Depth Map Inference

At inference time, given a new input image [formula], we compute its global depth map G by finding the sparse depth weights [formula] that best fit the image-based prediction, i.e., by solving the following optimization problem subject to positivity constraints on weights:

[formula]

The global depth map is then generated as G  =  B.

Empirically, we have found beneficial to apply the colorization procedure described in [\cite=Dani04colorization] to the global estimate G produced by our approach. This technique has been used in previous work [\cite=Silberman:ECCV12] [\cite=EigenPF14] to fill-in missing values in data collected by depth sensors. Here instead we use it to make the depth map more spatially coherent, as the colorization procedure encourages pixels having similar color to be mapped to similar values of depth. To do this, we first resize the low-resolution depth map G via bilinear interpolation to an intermediate size of PI pixels. Then we apply the colorization procedure using all pixels at this resolution as "color" propagation seeds with a low penalty value (the penalty values indicates how much the colorized depth values can deviate from the original input value).

Local Depth Refinement

The local depth refinement uses the prediction G from our global depth model (described in the previous section) and generates a higher resolution, locally-refined depth map [formula] containing finer details. Let [formula] be the global depth estimate resized to the intermediate resolution PI and post-processed via colorization as described in subsection [\ref=sec:globalinference]. Also, we denote with [formula] the ground truth depth map Di resized to size PI and vectorized. We propose to train a local refinement model that predicts the depth of pixel j in example i using a local descriptor [formula] computed at pixel j, i.e., [formula], where [formula] is a row vector encoding the model parameters. Note that this parameter vector is shared across pixels but, unlike our global depth estimator, the refinement predicts the depth at a pixel using as input a local descriptor computed at that pixel rather than the whole image. Specifically, we choose

[formula]

where [formula] is the depth estimate for pixel j in image i from our global model, which is used as additional feature here in order to guide the local refinement. Thus, the global depth estimate acts in a sense as a prior for the local refinement, which lacks the context of the full-image. In our experiments we show that providing [formula] as feature is critically necessary to achieve good accuracy in the local refinement. The first feature entry is set constant to 1 in order to implement the bias term. Finally, the features [formula] are radial basis functions computed with respect to [formula] centers. Note that while the radial basis functions for the global model were defined in terms of deep features f() computed from the whole image, global features are clearly not appropriate for the local refinement. Instead, we propose to use the hypercolumn feature vector [\cite=BharathCVPR2015] at pixel j, i.e., the activation values at location j in the convolutional feature maps of the deep PLACES network [\cite=zhou2014places] all stacked into a single vector. In practice, we use only layers pool2, conv4 and conv5, which gives rise to a hypercolumn vector of dimensionality 896 for each pixel. This representation has been shown to be able to simultaneously capture localized low-level visual information (from the early layers) as well as high-level semantics (from the deepest layers). Thus, it is very useful for localized, high-level visual analysis, such as our task of local depth refinement. More formally, we compute the radial basis features as [formula] where [formula] denotes the function that extracts the hypercolumn representation at pixel j and [formula] is the k-th center, itself a hypercolumn feature vector. As discussed in further detail in section [\ref=implementationdetails], the centers [formula] are the centroids computed by k-means over a training set of hypercolumn feature vectors.

The parameter vector [formula] is learned via simple regularized least-squares estimation on the training data:

[formula]

where the first two entries of [formula] (corresponding to the bias and the global depth prediction) are left unregularized.

At test time, given the input image [formula] and its global depth estimate [formula], we obtain the locally-refined depth value [formula] at pixel j as [formula]. Finally, we take this depth estimate at the intermediate resolution (PI pixels), resize it to the full resolution (R  ×  C) using bilinear interpolation and apply once more the colorization scheme, in order to render the final output more spatially coherent.

It is important to note that besides the use of local information (rather than the context from the full image), another fundamental difference between our global depth estimation and the refinement lies in the fact that the latter directly regresses on depth, while the former predicts depth reconstruction weights (i.e., the vector [formula]). This is consistent with the distinct objectives of the two steps: the global estimate takes advantage of the basis constraint to yield a robust but coarse estimate of the depth map; the local refinement can leverage the global depth estimate as a strong feature and thus can model the depth at individual pixels in an unconstrained fashion.

Implementation details

In this section we provide additional implementation details concerning our approach. To learn the global depth model, we downsample the training depth maps from 427  ×  561 to size 32  ×  43 for NYUv2 and from 256  ×  1242 to 32  ×  156 for KITTI (in order to maintain aspect ratio of ground truth) via bilinear interpolation. The sizes were chosen to reduce the dimensionality sufficiently so as to allow training of basis to happen without overfitting while at the same time producing a coarse depth map that contains meaningful information. Furthermore we subtract the per pixel mean from each of the depth maps so as to force our model to predict the deviations from the mean depth map. At inference time we add the mean depth to our depth estimate to get the final prediction. As intermediate resolution PI, we use 128  ×  172 for NYUv2 and 64  ×  311 for KITTI. Evaluations of estimated depth from both GCL and RCL are done by upsampling the output to full resolution.

In [\cite=Krizhevsky] the deep network was applied to multiple crops of the image and the predictions on the individual crops were then averaged. Inspired by this approach, we defined five distinct image crops (Center (C), Upper Left (UL), Upper Right (UR), Down Left (DL), Down Right (DR)) of size 227  ×  227 and we learned a distinct global model for each of the crops. However, note that all 5 models are trained to predict the complete depth map (thereby estimating also depth at pixels not in the crop). At inference, we generate the final depth at each pixel as a weighted average of the predictions from the 5 crops. We use spatially-varying weighting function of the 5 estimates at the coarse size. The weight of crop i at pixel location p is computed as [formula] where pi is the center pixel of crop i. Thus, at each pixel we give more importance to the predictions of crops that are closer to the pixel.

For each crop, we form the vector centers j used in the radial basis functions φj() by taking image examples from the two nearest crops. We use (UL,UR) as centers for C, (C,UR) as centers for UL, (C,UL) as centers for UR, (C,DR) as centers for DL and (C,DL) as centers for DR. We double the number of centers by including also the mirrored version of each crop in the kernel vector. For NYUv2, as the number of training examples is small (795) we use all training images as centers, thus the RBF kernel vector of each crop contains a total of 795  ×  2  ×  2  =  3180 centers (mirrored and un-mirrored version of each of the 2 closest crops for all 795 images). For KITTI, since the training set is in this case much larger (19,852 images), we use only a subset of it to create the RBF vector: specifically, for each crop we form the centers with the 654 examples that were used to train [\cite=saxena2009make3d], once again by choosing the mirrored and unmirrored versions of the 2 closest crops for all these images (this yields a total of 654  ×  2  ×  2  =  2616 centers for each crop). The σj in the kernel is set to be half of the maximum pairwise distance between centers.

For refinement, instead of learning a single shared model for all pixels of the image, we trained a separate pixel-based model for each block of 16 rows of the image (for a total of 8 distinct models). This is motivated by the observation that pixels within a row (or in neighboring rows) of the image tend to have similar depth statistics but pixels coming from distant rows often exhibit large depth variations, as already noted in [\cite=saxena2005learningdepth]. This is merely a consequence of ceilings being typically at the top of the image, walls in the middle and floors at the bottom of the picture. Each model is trained with a 512 dimensional RBF kernel-vector augmented with the global depth estimate [formula] and the constant feature 1. The 512 RBF centers for each block of rows are the k-means cluster centroids obtained by clustering randomly sampled pixels from that block of rows in the training set. Since using the global depth estimates on the training set would overfit the data and generate biased estimate of the feature [formula], we performed a 10-fold cross validation on the training set and used the global depth estimates predicted on each validation fold to generate the features for the subsequent training of the refinement. For each fold, we apply the procedure of training 5 different crops and merging outputs.

Experiments

In this work we apply our proposed approach to the NYUv2 [\cite=Silberman:ECCV12] and KITTI [\cite=Geiger2013IJRR] datasets and show that it produces state of the art results on depth estimation for both. These two datasets are dramatically different and serve well the objective of showing that our approach works for both indoor and outdoor settings.

Depth estimation can be quantitatively assessed according to different criteria. In this work, we report results on multiple metrics that are widely used: RMSE [\cite=EigenPF14], Absolute Relative error [\cite=EigenPF14], Scale Invariant error [\cite=EigenPF14], Threshold error [\cite=Ladicky_2014_CVPR], Log10 error [\cite=saxena2009make3d]. All of the metrics are evaluated at the full resolution of the ground truth depth maps. This means that the output of both GCL and RCL is upsampled to full resolution before evaluation. This allows us to compare the global and the refined estimates on the same ground.

The rest of this section is organized as follows: in §[\ref=expNYUv2] we present results of our models on NYUv2 and compare them to the state-of-the-art; in §[\ref=expKITTI] we discuss our experiments on KITTI; finally, in .3 we describe experimental results obtained by varying our model design choices, thus providing further empirical justification for our approach and the settings used in §[\ref=expNYUv2] and §[\ref=expKITTI].

NYUv2

The NYUv2 dataset [\cite=Silberman:ECCV12] consists of RGBD examples from 27 different indoor scene categories taken from a total of 464 different scenes. We evaluate our methods using the standard train/test split provided by the authors of NYUv2 (795 training examples, 654 test examples) [\cite=Silberman:ECCV12]. For the global method we use a depth basis B consisting of m = 48 atoms. Figure [\ref=fig:coarsedict] (left) shows the learned basis.

We begin by presenting in Table [\ref=tb:visResultsNYUV2] our depth reconstructions for a few sample images of this dataset. Notice how both GCL and RCL provide accurate depth estimates similar to the ground truth. But RCL tends to recover additional fine details (e.g., the bed corner and the various objects in the background of the first picture).

Next we compare our approach on this benchmark with published state-of-the-art methods. The results are summarized in Table [\ref=tb:ResultsNYUV2]. We denote our global estimation method as GCL (Global Coupled Learning) and our refinement procedure as RCL (Refined Coupled Learning). The last column reports the performance obtained by simply predicting the constant average depth map (computed from the training set) for any input, as this is an interesting baseline revealing the difficulty of the dataset. As can be seen, both our models outperform all prior methods by a large margin on the RMSE (the metric we optimize for) while remaining super competitive with new approaches if not better. We would like to note that the ideas explored in [\cite=CVPR15bFayao] are orthogonal to our line of investigation, any good features and CRF learnt from their method can easily be incorporated into our framework to produce even better results.

Note that we did not include in Table [\ref=tb:ResultsNYUV2] the results of the recent method proposed by Eigen et al. [\cite=EigenPF14] and Wang et al.[\cite=Wang_2015_CVPR] as these approaches were not trained on the standard training split of NYUv2. Both of these approaches use a training set that is 150 times larger than the one we used in this work (only 795 images). Despite this, our method achieves much lower RMSE error (0.8025 versus 0.871 for [\cite=EigenPF14] and 0.8371 for [\cite=Wang_2015_CVPR]) on the same test set.

Training on Large Scale NYU

In this section we describe results obtained by training our complete model on the large scale NYU training set recently released by Eigen et al. [\cite=EigenPF14]. This dataset is roughly 150 times bigger than the standard training set of the NYUv2 benchmark. All learned models are evaluated on the test set of NYUv2. Table [\ref=tb:eigenvsus] provides a quantitative analysis of results according to different metrics. We evaluate the performance of our approach compared to other deep learning approaches[\cite=EigenPF14] [\cite=Wang_2015_CVPR] which empahsis on feature learning.

We perform this experiment to illustrate two interesting features of our proposed approach. Firstly, whereas other methods of [\cite=EigenPF14] [\cite=Wang_2015_CVPR] cannot be trained on the standard training set without massive overfitting, our approach performs well even with the standard training set. Secondly, our approach is able to utilize the additional examples to improve on all metrics over the model learned from the standard train/test split.

In Table [\ref=tb:GCLvsLSGCL] we show the qualitative improvement enabled by the larger training set over training with the standard training set.

Generalizability of Approach

In order to demonstrate the generality of our approach, we tested our method on the task of surface normal estimation by training and applying the technique described in this paper to the problem of surface normal estimation. Table [\ref=tb:visResultsNYUV2Normals] shows qualitatively the results achieved by our approach on the NYUv2 dataset. The results were produced without any tweaking of hyperparameters to improve results for surface normal estimation. Quantiatively RCL leverages GCL to improve results dramatically reducing median error by over 5.5 degrees and increasing the percentage of pixels with error less than 11.25 degrees by 6

KITTI

The KITTI dataset is an outdoor scene dataset consisting of videos taken from a driving vehicle with depth provided by a LiDaR sensor at certain frames of the videos. On this dataset we used the train/test split proposed by Eigen et al.[\cite=EigenPF14] consisting of 19852 training examples and 697 test examples. The training and test sets include examples from the "city", "residential" and "road" sequences. For evaluation on this dataset, we use the same experimental setup as adopted by [\cite=EigenPF14].

We train our global depth model using a basis B consisting of m = 96 atoms. Figure [\ref=fig:coarsedict] (right) shows the learned basis for the center crop (for visualization purposes we show only 48 randomly selected atoms out of the 96 learned). Once again, we compare against the ground truth by resizing our estimates to full resolution.

Table [\ref=tb:ResultsKITTI] shows the results of our global model versus Eigen et al. [\cite=EigenPF14] (because KITTI is a recent dataset we could not find any other prior work using this training/test split to include in the comparison). Note that for KITTI, the model in [\cite=EigenPF14] was trained without the advantageous use of additional examples (unlike in the case of NYUv2). The Table shows that, given the same training data, our approach achieves higher accuracy according to the RMSE and the Threshold metric, while it is close to [\cite=EigenPF14] on the Relative and Scale-Invariant metrics.In Table [\ref=tb:visKITTI] we show the qualitative performance of our approach on the KITTI[\cite=Geiger2013IJRR] dataset.

Revisiting Model Design Choices

Global Estimation

In this section we study the impact of various design choices made in our global approach. Table [\ref=tb:coarseExp] summarizes this comparative study of different variants of our model on both NYUv2 as well as KITTI. In this work we proposed that in order to capture the structure in the output space (depth spatial smoothness, rejection of unlikely depth maps), it is beneficial to learn to predict reconstructive depth basis weights rather than regressing on depth directly. The second column of Table [\ref=tb:coarseExp] (Direct Regr) shows the performance obtained by learning a mapping that uses our kernel-based image features () to directly regress on the depth [formula]. Eliminating the basis model and regressing depth directly causes a large increase in RMSE error on both datasets. Another assumption in our approach is that coupling the learning of the basis and the regression provides a beneficial effect as it allows the method to optimize the depth representation for accurate prediction. Our hypothesis is confirmed by the results shown in the third column of Table [\ref=tb:coarseExp] (Uncoupled), which reports the performance obtained by learning a dictionary via sparse coding and then regressing on the weights of the dictionary. There is a clear degradation in accuracy on both datasets when the modeling of depth and the regression optimization are uncoupled.

Next, we assess which deep features are better for predicting depth. We consider two types of features, both extracted from the same deep network architecture [\cite=Krizhevsky] but trained on two different datasets: GCL uses "pool5" features trained on PLACES [\cite=zhou2014places], while GCL-I (last column of Table [\ref=tb:coarseExp]) uses "pool5" optimized for object class recognition on Imagenet [\cite=imagenet_cvpr09]. Our results show that features learned for scene classification perform much better on depth estimation compared to features trained for object classification.

Local Depth Refinement

Here we present experiments that shed light on the role of different components of our local depth refinement (RCL).

First, we assess the advantage of training separate models for different row-blocks of the image. As discussed, for RCL we subdivided the image into 8 non-overlapping blocks of 16 rows and trained a distinct model for each block. We now take a look at the impact of using a single model as opposed to the multi-model setting. In order to construct an equally powerful single model, we construct a [formula]-sized RBF descriptor to train the single-model regressor. However, we found that this yields consistently inferior results compared to the multi-model, e.g., the RMSE on NYUv2 is 0.8213 versus the 0.8025 of RCL.

In order to show the importance of estimating the global depth before the local refinement, we tried training a variant of RCL that does not include the global estimate [formula] in the feature vector of Eq. [\ref=eq:localfeature]. Effectively this model uses only the local hypercolumn vector to directly regress the depth of each pixel. This results in dramatically worse accuracy: the RMSE error on NYUv2 is 1.1211 instead to 0.8025! This furthers validates our belief that a good method for depth estimation requires a really strong global model.

Analysis of Computational Cost

We now show that our approach is both scalable and extremely fast to train. We compare the computational cost of our approach to methods such as [\cite=EigenPF14] [\cite=Wang_2015_CVPR] [\cite=CVPR15bFayao], which produce the best published results on two variants of the NYUv2 dataset. [\cite=CVPR15bFayao]'s approach makes use of the standard training set (795 examples) for learning a deep network which takes them 33 hours even when using a GPU. In contrast, our global framework (GCL) requires approximately 15 minutes for feature extraction of the standard train/test split NYUv2 dataset and 10 minutes for learning all 5 models on a Xeon E5 CPU! [\cite=EigenPF14] [\cite=Wang_2015_CVPR] use 136,847 and 200,000 training examples respectively to train deep networks. The coarse model in [\cite=EigenPF14] is trained in 38 hours, whereas the model in [\cite=Wang_2015_CVPR] takes 4 days to train and that too on high end GPUs! In the large scale setting takes a total of 8 hours to learn models from 136,847 examples.

For refinement, the method proposed in [\cite=EigenPF14] takes 26 hours to train. In comparison our refinement method (RCL) requires [formula] hour to be trained (including time taken for k-mean clustering to compute the RBF centroids). We train these independent models in parallel using a cluster, which makes the total training still [formula] hour. RCL inference takes 8 second per image.

Conclusion

We presented a novel approach to depth prediction from single image that naturally integrates global and local information. Global cues in the form of deep convolutional features are used to predict the depth map. In a subsequent stage the estimated global map is used to guide a local refinement at a higher resolution. Global estimation is formulated as the joint learning of a depth basis and a regression mapping from the image space to the basis weights. The local refinement regresses directly on pixel depth using the global estimate as feature. Our approach yields an improvement over the state-of-the-art on the standard train/test split of the NYUv2 and KITTI datasets and is fast and scalable. Future work will involve integrating feature learning in the framework of coupled regression and modeling of depth and also exploring joint depth and surface normal estimation in this framework.