Iterative Instance Segmentation

Introduction

In computer vision, the objective of many tasks is to predict a pixel-wise labelling of the input image. While the intrinsic structure of images constrains the space of sensible labellings, existing approaches typically eschew leveraging such cues and instead predict the label for each pixel independently. Consequently, the resulting predictions may not be visually plausible. To mitigate this, a common strategy is to perform post-processing on the predictions using superpixel projections [\cite=hypercolumn] or conditional random fields (CRFs) [\cite=KrahenbuhlNIPS2011], which ensures the final predictions are consistent with local appearance cues like colour and texture but fails to account for global object-level cues like shape.

Despite its obvious shortcomings, this strategy enjoys popularity, partly because incorporating global cues requires introducing higher-order potentials in the graphical model and often makes inference intractable. Because inference in general graphical models is NP-hard, extensive work on structured prediction has focused on devising efficient inference algorithms in special cases where the higher-order potentials take on a particular form. Unfortunately, this restricts the expressive power of the model. As a result, care must be taken to formulate the cues of interest as higher-order potentials of the desired form, which may not be possible. Moreover, low-energy configurations of the potentials often need to be specified manually a priori, which may not be practical when the cues of interest are complex and abstract concepts like shape.

In this paper, we devise a method that learns implicit shape priors and use them to improve the quality of the predicted pixel-wise labelling. Instead of attempting to capture shape using explicit constraints, we would like to model shape implicitly and allow the concept of shape to emerge from data automatically. To this end, we draw inspiration from iterative approaches like auto-context [\cite=auto], inference machines [\cite=RossCVPR2011] and iterative error feedback (IEF) [\cite=ief]. Rather than learning a model to predict the target in one step, we decompose the prediction process into multiple steps and allow the model to make mistakes in intermediate steps as long as it is able to correct them in subsequent steps. By learning to correct previous mistakes, the model must learn the underlying structure in the output implicitly in order to use it to make corrections.

To evaluate if the method is successful in learning shape constraints, a perfect testbed is the task of instance segmentation, the goal of which is to identify the pixels that belong to each individual object instance in an image. Because the unit of interest is an object instance rather than an entire object category, methods that leverage only local cues have difficulty in identifying the instance a pixel belongs to in scenes with multiple object instances of the same category that are adjacent to one another, as illustrated in Figure [\ref=fig:intro]. We demonstrate that the proposed method is able to successfully learn a category-specific shape prior and correctly suppresses pixels belonging to other instances. It is also able to automatically discover a prior favouring contiguity of region predictions and smoothness of region contours despite these being not explicitly specified in the model. Quantitatively, it outperforms the state-of-the-art and achieves a mean [formula] of 63.7% at 50% overlap and 42.2% at 70% overlap.

Related Work

Yang et al. [\cite=YangTPAMI2012] first described the task of segmenting out individual instances of a category. The metrics we use in this paper were detailed by Tighe et al. [\cite=TigheCVPR2014], who proposed non-parametric transfer of instance masks from the training set to detected objects, and by Hariharan et al. [\cite=BharathECCV2014] who used convolutional neural nets (CNNs)  [\cite=LecunNC1989] to classify region proposals. We use the terminology and metrics proposed by the latter in this paper. Dai et al. [\cite=DaiCVPR2015] used ideas from [\cite=HeECCV2014] to speed up the CNN-based proposal classification significantly.

A simple way of tackling this task is to run an object detector and segment out each detected instance. The notion of segmenting out detected objects has a long history in computer vision. Usually this idea has been used to aid semantic segmentation, or the task of labeling pixels in an image with category labels. Borenstein and Ullman [\cite=borenstein2002class] first suggested using category-specific information to improve the accuracy of segmentation. Yang et al. [\cite=YangTPAMI2012] start from object detections from the deformable parts model [\cite=FelzenszwalbPAMI2010] and paste figure-ground masks for each detected object. Similarly, Brox et al. [\cite=BroxCVPR2011] and Arbeláez et al. [\cite=ArbelaezCVPR2012] paste figure-ground masks for poselet detections [\cite=BourdevECCV2010]. Recent advances in computer vision have all but replaced early detectors such as DPM and poselets with ones based on CNNs [\cite=LecunNC1989] [\cite=GirshickCVPR2014] [\cite=GirshickICCV2015] and produced dramatic improvements in performance in the process. In the CNN era, Hariharan et al. [\cite=hypercolumn] used features from CNNs to segment out R-CNN detections [\cite=GirshickCVPR2014].

When producing figure-ground masks for detections, most of these approaches predict every pixel independently. However, this disregards the fact that pixels in the image are hardly independent of each other, and a figure-ground labeling has to satisfy certain constraints. Some of these constraints can be simply encoded as local smoothness: nearby pixels of similar color should be labeled similarly. This can be achieved simply by aligning the predicted segmentation to image contours [\cite=BroxCVPR2011] or projecting to superpixels [\cite=hypercolumn]. More sophisticated approaches model the problem using CRFs with unary and pairwise potentials  [\cite=RotherTOG2004] [\cite=ParkhiICCV2011] [\cite=KrahenbuhlNIPS2011]. Later work consider extending these models by incorporating higher-order potentials of specific forms for which inference is tractable [\cite=kohli2013principled] [\cite=li2013exploring]. Zheng et al. [\cite=ZhengArxiv2015] show that inference in CRFs can be viewed as recurrent neural nets and trained together with a CNN to label pixels, resulting in large gains. Another alternative is to use eigenvectors obtained from normalized cuts as an embedding for pixels [\cite=MajiCVPR2011] [\cite=MaireICCV2011].

However, images contain more structure than just local appearance-dependent smoothness. For instance, one high informative form of global cue is shape; in the case of persons, it encodes important constraints like two heads cannot be part of the same person, the head must be above the torso and so on. There has been prior work on handling such constraints in the pose estimation task by using graphical models defined over keypoint locations [\cite=YangTPAMI2013] [\cite=TompsonNIPS2014]. However, in many applications, keypoint locations are unknown and such constraints must be enforced on raw pixels. Explicitly specifying these constraints on pixels is impractical, since it would require formulating potentials that are capable of localizing different parts of an object, which itself is a challenging task. Even if this could be done, the potentials that are induced would be higher order (which arises from the relative position constraints among multiple parts of an object) and non-submodular (due to mutual exclusivity constraints between pixels belonging to two different heads). This makes exact inference and training in these graphical models intractable.

Auto-context [\cite=auto] and inference machines [\cite=RossCVPR2011] take advantage of the observation that performing accurate inference does not necessarily require modelling the posterior distribution explicitly. Instead, these approaches devise efficient iterative inference procedures that directly approximate message passing. By doing so, they are able to leverage information from distant spatial locations when making predictions while remaining computationally efficient. In a similar spirit, other methods model the iterative process as recurrent neural nets  [\cite=PinheiroICML2014] [\cite=ZhengArxiv2015]. IEF [\cite=ief] uses a related approach on the task of human pose estimation by directly refining the prediction rather than approximating message passing in each iteration. While this approach shows promise when the predictions lie in a low-dimensional space of possible 2D locations of human joints, it is unclear if it will be effective when the output is high-dimensional and embeds complex structure like shape, as is the case with tasks that require a pixel-wise labelling of the input. In this paper, we devise an iterative method that supports prediction in high-dimensional spaces without a natural distance metric for measuring conformity to structure.

Method

Task and Setting

The objective of the instance segmentation task, also known as simultaneous detection and segmentation (SDS), is to predict the segmentation mask for each object instance in an image. Typically, an object detection system is run in the first stage of the pipeline, which generates a set of candidate bounding boxes along with the associated detection scores and category labels. Next, non-maximum suppression (NMS) is applied to these detections, which are then fed into the segmentation system, which predicts a heatmap for each bounding box representing the probability of each pixel inside the bounding box belonging to the foreground object of interest. The heatmaps then optionally undergo some form of post-processing, such as projection to superpixels. Finally, they are binarized by applying a threshold, yielding the final segmentation mask predictions. We use fast R-CNN [\cite=GirshickICCV2015] trained on MCG [\cite=arbelaez2014multiscale] bounding box proposals as our detection system and focus on designing the segmentation system in this paper.

Segmentation System

For our segmentation system, we use a CNN that takes a 224  ×  224 patch as input and outputs a 50  ×  50 heatmap prediction. The architecture is based on that of the hypercolmumn net proposed by Hariharan et al. [\cite=hypercolumn], which is designed to be sensitive to image features at finer scales and relative locations of feature activations within the bounding box. Specifically, we use the architecture based on the VGG 16-layer net [\cite=simonyan2014very] (referred to as "O-Net" in [\cite=hypercolumn]), in which heatmaps are computed from the concatenation of upsampled feature maps from multiple intermediate layers, known as the hypercolumn representation. The CNN is trained end-to-end on the PASCAL VOC 2012 training set with ground truth instance segmentation masks from the Semantic Boundaries Dataset (SBD) [\cite=sbd] starting from an initialization from the weights of a net finetuned for the detection task using R-CNN [\cite=GirshickCVPR2014].

Algorithm

We would like to incorporate global cues like shape when making predictions. Shape encodes important structural constraints, such as the fact that a person cannot have two heads, which is why humans are capable of recognizing the category of an object from its silhouette almost effortlessly. So, leveraging shape enables us to disambiguate region hypotheses that all correctly cover pixels belonging to the category of interest but may group pixels into instances incorrectly.

Producing a heatmap prediction that is consistent with shape cues is a structured prediction problem, with the structure being shape constraints. The proposed algorithm works by reducing the structured prediction problem to a sequence of unconstrained prediction problems. Instead of forcing the model to produce a prediction that is consistent with both the input and the structure in a single step, we allow the model to disregard structure initially and train it to correct its mistakes arising from disregarding structure over multiple steps, while ensuring consistency of the prediction with the input in each step. The final prediction is therefore consistent with both the input and the structure. Later, we demonstrate that this procedure is capable to learning a shape prior, a contiguity prior and a contours smoothness prior purely from data without any a priori specification to bias the learning towards finding these priors.

At test time, in each step, we feed the input image and the prediction from the previous step, which defaults to constant prediction of 1 / 2 in the initial step, into the model and take the prediction from the last step as our final prediction. In our setting, the model takes the form of a CNN. Please see Figure [\ref=fig:teaser] for a conceptual illustration of this procedure.

Training the model is straightforward and is done in stages: in the first stage, the model is trained to predict the ground truth segmentation mask with the previous heatmap prediction set to 1 / 2 for all pixels and the predictions of the model at the end of training are stored for later use. In each subsequent stage, the model is trained starting from the parameters values at the end of the previous stage to predict the ground truth segmentation mask from the input image and the predictions generated in all previous stages.

Pseudocode of the training and testing procedures are shown in Algorithms [\ref=train_alg] and [\ref=test_alg].

Discussion

Modelling shape constraints using traditional structured prediction approaches would be challenging for three reasons. First, because shape is a highly abstract concept, it is difficult to explicitly formulate the set of structural constraints it imposes on the output. Furthermore, even if it could be done, manual specification would introduce biases that favour human preconceptions and lead to inaccuracies in the predictions. Therefore, manually engineering the form of structural constraints is neither feasible or desirable. Hence, the structural constraints are unknown and must be learned from data automatically. Second, because shape imposes constraints on the relationship between different parts of the object, such as the fact that a person cannot have two heads, it is dependent on the semantics of the image. As a result, the potentials must be capable of representing high-level semantic concepts like "head" and would need to have complex non-linear dependence on the input image, which would complicate learning. Finally, because shape simultaneously constrains the labels of many pixels and enforce mutual exclusivity between competing region hypotheses, the potentials would need to be of higher order and non-submodular, often making inference intractable.

Compared to the traditional single-step structured prediction paradigm, the proposed multi-step prediction procedure is more powerful because it is easier to model local corrections than the global structure. This can be viewed geometrically - a single-step prediction procedure effectively attempts to model the manifold defined by the structure directly, the geometry of which could be very complex. In contrast, our multi-step procedure learns to model the gradient of an implicit function whose level set defines the manifold, which tends to have much simpler geometry. Because it is possible to recover the manifold, which is a level set of an implicit function, from the gradient of the function, learning the gradient suffices for modelling structure.

Implementation Details

We modify the architecture introduced by Hariharan et al. [\cite=hypercolumn] as follows. Because shape is only expected to be consistent for objects in the same category, we make the weights of the first layer category-dependent by adding twenty channels to the input layer, each corresponding to a different object category. The channel that corresponds to the category given by the detection system contains the heatmap prediction from the previous step, and channels corresponding to other categories are filled with zeros. To prepare the input to the CNN, patches inside the bounding boxes generated by the detection system are extracted and anisotropically scaled to 224  ×  224 and the ground truth segmentation mask is transformed accordingly. Because the heatmap prediction from the preceding step is 50  ×  50, we upsample it to 224  ×  224 using bilinear interpolation before feeding it in as input. To ensure learning is well-conditioned, the heatmap prediction is rescaled and centred element-wise to lie in the range

[formula]

Evaluation

We evaluate the proposed method in terms of region average precision ([formula]), which is introduced by [\cite=sds]. Region average precision is defined in the same way as the standard average precision metric used for the detection task, with the difference being the computation of overlap between the prediction and the ground truth. For instance segmentation, overlap is defined as the pixel-wise intersection-over-union (IoU) of the region prediction and the ground truth segmentation mask, instead of the IoU of their respective bounding boxes. We evaluate on the PASCAL VOC 2012 validation set and demonstrate that the proposed method outperforms the state-of-the-art, achieving a mean [formula] of 63.7% at 50% overlap and 42.2% at 70% overlap.

Experiments

First, we visualize the improvement in prediction accuracy as training progresses. In Figure [\ref=fig:ief_training], we show the pixel-wise heatmap predictions on image patches from the PASCAL VOC 2012 validation set after each stage of training. As shown, prediction quality steadily improves with each successive stage of training. Initially, the model is only able to identify some parts of the object; with each stage of training, it learns to recover additional parts of the object that were previously missed. After four stages of training, the model is able to correctly identify most parts belonging to the object. This indicates that the model is able to learn to make local corrections to its predictions in each stage. After four stages of training, the predictions are reasonably visually coherent and consistent with the underlying structure of the output space. Interestingly, the model gradually learns to suppress parts of other objects, as shown by the predictions on the bicycle and horse images, where the model learns to suppress parts of the pole and the other horse in later stages.

Next, we compare the performance of the proposed method with the state-of-the-art method, the hypercolumn net [\cite=hypercolumn], under three settings: without superpixel projection, with superpixel projection and with superpixel projection and rescoring. As shown in Table [\ref=tab:map_comp], the proposed method outperforms the state-of-the-art in each setting in terms of mean [formula] at both 50% and 70%. In particular, the proposed method achieves an 8.3-point gain over the state-of-the-art in terms of its raw pixel-wise prediction performance at 70% overlap. This indicates the raw heatmaps produced by the proposed method are more accurate than those produced by the vanilla hypercolumn net. As a result, the proposed method requires less reliance on post-processing. We confirm this intuition by visualizing the heatmaps in Figure [\ref=fig:preds]. When superpixel projection is applied, the proposed method improves performance by 1.7 points and 3.8 points at 50% and 70% overlaps respectively. With rescoring, the proposed method obtains a mean [formula] of 63.7% at 50% overlap and 42.2% at 70% overlap, which represent the best performance on the instance segmentation task to date. We break down performance of the proposed method by category under each setting in Tables [\ref=tab:categ_specific_res_50] and [\ref=tab:categ_specific_res_70].

We examine heatmap and region predictions of the proposed method and the vanilla hypercolumn net, both with and without applying superpixel projection. As shown in Figure [\ref=fig:preds], the pixel-wise heatmap predictions produced by the proposed method are generally more visually coherent than those produced by the vanilla hypercolumn net. In particular, the proposed method predicts regions that are more consistent with shape. For example, the heatmap predictions produced by the proposed method for the sportscaster and the toddler images contain less noise and correctly identify most foreground pixels with high confidence. In contrast, the heatmap predictions produced by the hypercolumn net are both noisy and inconsistent with the typical shape of persons. On the bicycle image, the proposed method is able to produce a fairly accurate segmentation, whereas the hypercolumn net largely fails to find the contours of the bicycle. On the horse image, the proposed method correctly identifies the body and the legs of the horse. It also incorrectly hallucinates the head of the horse, which is actually occluded; this mistake is reasonable given the similar appearance of adjacent horses. This effect provides some evidence that the method is able to learn a shape prior successfully; because the shape prior discounts the probability of seeing a headless horse, it causes the model to hallucinate a head. On the other hand, the hypercolumn net chooses to hedge its bets on the possible locations of the head and so the resulting region prediction is noisy in the area near the expected location of the head. Notably, the region predictions generated by the proposed method also tend to contain fewer holes and have smoother contours than those produced by the hypercolumn net, which is apparent in the case of the sportscaster and toddler images. This suggests that the model is able to learn a prior favouring the contiguity of regions and smoothness of region contours.

Applying superpixel projection significantly improves the region predictions of the vanilla hypercolumn net. It effectively smoothes out noise in the raw heatmap predictions by averaging the heat intensities over all pixels in a superpixel. As a result, the region predictions contain fewer holes after applying superpixel projection, as shown by the predictions on the sportscaster and toddler images. Superpixel projection also ensures that the region predictions conform to the edge contours in the image, which can result in a significant improvement if the raw pixel-wise region prediction is very poor, as is the case on the bicycle image. On the other hand, because the raw pixel-wise predictions of the proposed method are generally less noisy and have more accurate contours than those of the hypercolumn net, superpixel projection does not improve the quality of predictions as significantly. In some cases, it may lead to a performance drop, as pixel-wise prediction may capture details that are missed by the superpixel segmentation. As an example, on the bicycle image, the seat is originally segmented correctly in the pixel-wise prediction, but is completely missed after applying superpixel projection. Therefore, superpixel projection has the effect of masking prediction errors and limits performance when the quality of pixel-wise predictions becomes better than that of the superpixel segmentation.

We find that the proposed method is able to avoid some of the mistakes made by the vanilla hypercolumn net on images with challenging scene configurations, such as those depicting groups of people. On such images, the hypercolumn net sometimes includes parts of adjacent persons in region predictions. Two such examples are shown in Figure [\ref=fig:challenging_img], in which region predictions contain two heads from different people. The proposed method is able to suppress the heads of adjacent people and correctly exclude them from region predictions, suggesting that the shape prior that is learned is able to help the model disambiguate between the region hypotheses with one head and two heads, both of which are consistent with local appearance cues.

We now analyze the improvement in overlap between region predictions and the ground truth segmentation masks at the level of individual detections. In Figure [\ref=fig:overlap_comparison], we plot the maximum overlap of the pixel-wise region prediction produced by the proposed method with the ground truth against that of the region prediction generated by the vanilla hypercolumn net for each of the top 200 detections in each category. So, in this plot, any point above the diagonal represents a detection for which the proposed method produces a more accurate region prediction than the hypercolumn net. We find overlap with ground truth improves for 76% of the detections, degrades for 15.6% of the detections and remains the same for the rest. This is reflected in the plot, where the vast majority of the points lie above the diagonal, indicating that the proposed method improves the accuracy of region predictions for most detections.

Remarkably, for detections on which reasonably good overlap is achieved using the vanilla hypercolumn net, which tend to correspond to bounding boxes that are well-localized, the proposed method can improve overlap by ~ 15% in many cases. Furthermore, the increase in overlap tends to be the greatest for detections on which the hypercolumn net achieves ~ 75% overlap; when the proposed method is used, overlap for these detections at times reach more than 90%. This is particularly surprising given that improving upon good predictions is typically challenging. Such a performance gain is conceptually difficult to achieve without leveraging structure in the output and suggests that the proposed method is able to use the priors it learned to further refine region predictions that are already very accurate.

Finally, we conduct an experiment to test whether the proposed method is indeed able to learn a shape prior more directly. To this end, we select an image patch from the PASCAL VOC 2012 validation set that contains little visually distinctive features, so that it does not resemble an object from any of the categories. We then feed the patch into the model along with an arbitrary category label, which essentially forces the model to try to interpret the image as that of an object of the particular category. We are interested in examining if the model is able to hallucinate a region that is both consistent with the input image and resembles an object from the specified category.

Figure [\ref=fig:hallucinations] shows the input image and the resulting heatmap predictions under different settings of category. As shown, when the category is set to bird, the heatmap prediction resembles the body and the wing of a bird. When the category is set to horse, the model hallucinates the body and the legs of a horse. Interestingly, the wing of the bird and the legs of the horse are hallucinated even though there are no corresponding contours that resemble these parts in the input image. When the category is set to bicycle, the model interprets the edges in the input image as the frame of a bicycle, which contrasts with the heatmap prediction when the category is set to television, which is not sensitive to thin edges in the input image and instead contains a large contiguous box that resembles the shape of a television set.

Conclusion

We presented a method that is able to take advantage of the implicit structure that underlies the output space when making predictions. The method does not require manual specification of the form of the structure a priori and is able to discover salient structure from the data automatically. We apply the method to the simultaneous detection and segmentation task and show that the method automatically learns a prior on shape, contiguity of regions and smoothness of region contours. We also demonstrate state-of-the-art performance on instance segmentation using the method, which achieves a mean [formula] of 63.7% and 42.2% at 50% and 70% overlaps respectively. The method is generally applicable to all tasks that require the prediction of a pixel-wise labelling of the input image; we hope the success we demonstrated on instance segmentation will encourage application to other such tasks and further exploration of the method.

Supplementary Material

Below are the predictions of the proposed method and the vanilla hypercolumn net on additional images from various categories.